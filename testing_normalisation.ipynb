{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transmission Count by Category:\n",
      "Category A: 251 times\n",
      "Category B: 20 times\n",
      "\n",
      "Dynamic Penalty Statistics:\n",
      "Average Penalty: 0.6538\n",
      "Maximum Penalty: 0.6808\n",
      "Minimum Penalty: 0.0000\n",
      "\n",
      "Nodes Polled Statistics:\n",
      "Average Nodes Polled per Time Step: 0.03\n",
      "Maximum Nodes Polled at any Time Step: 10\n",
      "Target Maximum (M): 1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset and preprocess\n",
    "#pivot_df = pd.read_csv(\"synthetic_scenario_30_nodes.csv\")\n",
    "#pivot_df = pd.read_csv(\"simulated_office_environment.csv\")\n",
    "#pivot_df = pd.read_csv(\"synthetic_temp_polling_data.csv\")\n",
    "pivot_df = pd.read_csv(\"synthetic_temp_polling_data_reversed.csv\")\n",
    "\n",
    "pivot_df = pivot_df.apply(lambda x: x.fillna(x.mean()), axis=0)  # Fill missing values\n",
    "pivot_df = pivot_df.head(10000)  # Restrict to first 20000 time steps\n",
    "\n",
    "# Parameters\n",
    "reward = 0.5\n",
    "M = 1 # Maximum number of nodes that can be polled\n",
    "theta = 0.5  # Threshold for reward condition\n",
    "penalty = -0.5  # Penalty for polling when difference is <= theta\n",
    "initial_value = 20  # Initial estimate for last polled values\n",
    "\n",
    "# Set parameters\n",
    "beta_1 = 0.9  # dEWMA parameter for state value\n",
    "beta_2 = 0.9 # dEWMA parameter for rate of change\n",
    "\n",
    "# Extract column names, ensuring \"SN\" is excluded\n",
    "columns = [col for col in pivot_df.columns if col != \"SN\"]\n",
    "num_nodes = len(columns)  # Number of sensor nodes based on dataset columns\n",
    "num_time_steps = len(pivot_df)  # Total time steps based on dataset length\n",
    "\n",
    "# Track the number of times each category is pulled\n",
    "category_counts = {'Category A': 0, 'Category B': 0}\n",
    "\n",
    "# Function to calculate Age of Incorrect Information (AoII) at the sink\n",
    "def calculate_aoii_sink(current_time, last_received_time, last_rate_of_change):\n",
    "    # Handle potential NaN or Inf values\n",
    "    time_diff = current_time - last_received_time\n",
    "    if np.isnan(last_rate_of_change) or np.isinf(last_rate_of_change):\n",
    "        return 0.0  # Default to zero if rate of change is invalid\n",
    "    return abs(time_diff * last_rate_of_change)\n",
    "\n",
    "# Helper function to update state using dEWMA\n",
    "def update_node_state_dewma(measured_value, last_state_value, last_rate_of_change, delta_t, beta_1, beta_2):\n",
    "    # Handle the case where delta_t is 0 to avoid division by zero\n",
    "    if delta_t == 0:\n",
    "        return measured_value, last_rate_of_change  # Return measured value and keep the last rate of change\n",
    "    \n",
    "    x1 = beta_1 * measured_value + (1 - beta_1) * (last_state_value + last_rate_of_change * delta_t)\n",
    "    x2 = beta_2 * (x1 - last_state_value) / delta_t + (1 - beta_2) * last_rate_of_change\n",
    "    return x1, x2\n",
    "\n",
    "# Helper function to calculate reward\n",
    "def calculate_reward(measured_value, last_state_value, theta, penalty):\n",
    "    if abs(measured_value - last_state_value) > theta:\n",
    "        return reward  # Reward\n",
    "    return penalty  # Penalty\n",
    "\n",
    "# Function to extract numeric node ID from column names dynamically, ensuring valid extraction\n",
    "def extract_node_id(col_name):\n",
    "    \"\"\"Extract numeric node ID from column name, handling cases where no digits are found.\"\"\"\n",
    "    digits = ''.join(filter(str.isdigit, col_name))\n",
    "    return int(digits) if digits else None  # Return None if no digits are found\n",
    "\n",
    "# Dynamic Penalty Update algorithm based on Whittle indices\n",
    "def dynamic_penalty_update(whittle_indices, M, current_lambda):\n",
    "    # Convert dictionary values to a list of whittle indices, handling any NaN values\n",
    "    c_values = []\n",
    "    for v in whittle_indices.values():\n",
    "        if np.isnan(v):\n",
    "            c_values.append(-float('inf'))  # Treat NaN as negative infinity (won't be polled)\n",
    "        else:\n",
    "            c_values.append(v)\n",
    "    \n",
    "    # Identify the set ℰ of nodes where c_i > λ(t)\n",
    "    eligible_nodes = [i for i, c_i in enumerate(c_values) if c_i > current_lambda]\n",
    "    \n",
    "    # If |ℰ| ≤ M, no penalty update needed\n",
    "    if len(eligible_nodes) <= M:\n",
    "        return current_lambda\n",
    "    \n",
    "    # Sort the c_i values in descending order\n",
    "    sorted_c_values = sorted(c_values, reverse=True)\n",
    "    \n",
    "    # Identify the M-th value\n",
    "    M_th_value = sorted_c_values[M-1]\n",
    "    \n",
    "    # Update penalty to the M-th value\n",
    "    new_lambda = M_th_value\n",
    "    \n",
    "    return new_lambda\n",
    "\n",
    "# Main function to simulate Whittle AoII with rewards and track transmission counts\n",
    "def run_simulation_whittle_aoii_dynamic_penalty(pivot_df, columns, M, theta, penalty):\n",
    "    cumulative_reward = 0  # Track total cumulative reward\n",
    "    cumulative_rewards = []  # Store cumulative average reward over time\n",
    "    last_update_times = {col: 0 for col in columns}  # Last update time for each node\n",
    "    state_node = {col: np.array([20.0, 0.1]) for col in columns}  # Node states\n",
    "    \n",
    "    # Initialize dynamic penalty (λ) to 0\n",
    "    aoii_penalty = 0.0\n",
    "    \n",
    "    # Track penalty evolution\n",
    "    penalty_values = [aoii_penalty]\n",
    "    \n",
    "    # Track nodes polled at each time step\n",
    "    nodes_polled_count = []\n",
    "    \n",
    "    # Set minimum timestamp difference to avoid division by zero\n",
    "    min_delta_t = 1  # Minimum time difference of 1 to avoid division by zero\n",
    "\n",
    "    for t in range(len(pivot_df)):\n",
    "        # Step 1: Compute Whittle indices for each node based on AoII\n",
    "        whittle_indices = {}\n",
    "        for col in columns:\n",
    "            last_state_value, last_rate_of_change = state_node[col]\n",
    "            measured_value = pivot_df.loc[t, col]\n",
    "\n",
    "            # Correct AoII calculation at the sink using rate of change\n",
    "            current_aoii = calculate_aoii_sink(t, last_update_times[col], last_rate_of_change)\n",
    "            future_aoii_passive = calculate_aoii_sink(t+1, last_update_times[col], last_rate_of_change)\n",
    "            future_aoii_active = 0  # AoII resets to 0 if polled\n",
    "\n",
    "            # Whittle index calculations - with safety checks for NaN/Inf values\n",
    "            q_passive = current_aoii + future_aoii_passive\n",
    "            q_active = current_aoii + future_aoii_active + aoii_penalty\n",
    "            \n",
    "            # Calculate the Whittle index with safeguards against invalid values\n",
    "            whittle_index = q_passive - q_active\n",
    "            \n",
    "            # Handle NaN or Inf values\n",
    "            if np.isnan(whittle_index) or np.isinf(whittle_index):\n",
    "                whittle_indices[col] = -float('inf')  # Set to negative infinity if invalid\n",
    "            else:\n",
    "                whittle_indices[col] = whittle_index\n",
    "\n",
    "        # Step 2: Update the dynamic penalty (λ) using the algorithm\n",
    "        aoii_penalty = dynamic_penalty_update(whittle_indices, M, aoii_penalty)\n",
    "        penalty_values.append(aoii_penalty)\n",
    "        \n",
    "        # Step 3: Select nodes to poll based on the updated penalty\n",
    "        nodes_to_poll = [col for col in whittle_indices if whittle_indices[col] >= aoii_penalty]\n",
    "        nodes_polled_count.append(len(nodes_to_poll))\n",
    "\n",
    "        # Step 4: Poll selected nodes and calculate rewards\n",
    "        for col in nodes_to_poll:\n",
    "            measured_value = pivot_df.loc[t, col]\n",
    "            last_state_value, last_rate_of_change = state_node[col]\n",
    "\n",
    "            # Calculate reward after polling\n",
    "            reward = calculate_reward(measured_value, last_state_value, theta, penalty)\n",
    "            cumulative_reward += reward  # Update cumulative reward\n",
    "\n",
    "            delta_t_dynamic = max(min_delta_t, t - last_update_times[col])  # Time since last update (ensure at least 1)\n",
    "\n",
    "            # Update node state and last update time\n",
    "            state_node[col] = update_node_state_dewma(\n",
    "                measured_value, last_state_value, last_rate_of_change, delta_t_dynamic, beta_1=beta_1, beta_2=beta_2\n",
    "            )\n",
    "            last_update_times[col] = t\n",
    "\n",
    "            # Extract node ID dynamically and categorize\n",
    "            node_id = extract_node_id(col)\n",
    "            if node_id is not None:\n",
    "                if 1 <= node_id <= 5:\n",
    "                    category_counts['Category A'] += 1\n",
    "                elif 6 <= node_id <= 10:\n",
    "                    category_counts['Category B'] += 1\n",
    "             \n",
    "\n",
    "        # Step 5: Calculate cumulative average reward\n",
    "        cumulative_rewards.append(cumulative_reward / (t + 1))\n",
    "\n",
    "    return cumulative_rewards, category_counts, penalty_values, nodes_polled_count\n",
    "\n",
    "# Run the simulation with dynamic penalty\n",
    "cumulative_rewards_whittle, category_pulled_counts, penalty_values, nodes_polled_count = run_simulation_whittle_aoii_dynamic_penalty(\n",
    "    pivot_df, columns, M, theta, penalty\n",
    ")\n",
    "\n",
    "# Print the number of times each category was pulled\n",
    "print(\"Transmission Count by Category:\")\n",
    "for category, count in category_pulled_counts.items():\n",
    "    print(f\"{category}: {count} times\")\n",
    "\n",
    "# Save cumulative rewards to CSV\n",
    "pd.DataFrame(cumulative_rewards_whittle, columns=[\"cumulative_reward\"]).to_csv(\n",
    "    \"cumulative_rewards_whittle_dynamic_penalty.csv\", index=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Calculate statistics about the penalty values and nodes polled\n",
    "avg_penalty = np.mean(penalty_values)\n",
    "max_penalty = np.max(penalty_values)\n",
    "min_penalty = np.min(penalty_values)\n",
    "avg_nodes_polled = np.mean(nodes_polled_count)\n",
    "max_nodes_polled = np.max(nodes_polled_count)\n",
    "\n",
    "print(f\"\\nDynamic Penalty Statistics:\")\n",
    "print(f\"Average Penalty: {avg_penalty:.4f}\")\n",
    "print(f\"Maximum Penalty: {max_penalty:.4f}\")\n",
    "print(f\"Minimum Penalty: {min_penalty:.4f}\")\n",
    "print(f\"\\nNodes Polled Statistics:\")\n",
    "print(f\"Average Nodes Polled per Time Step: {avg_nodes_polled:.2f}\")\n",
    "print(f\"Maximum Nodes Polled at any Time Step: {max_nodes_polled}\")\n",
    "print(f\"Target Maximum (M): {M}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1 (t=1 to t=5000) Transmission Count by Category:\n",
      "Category A: 790 times (97.7%)\n",
      "Category B: 19 times (2.3%)\n",
      "\n",
      "Phase 2 (t=5001 to t=10000) Transmission Count by Category:\n",
      "Category A: 16 times (84.2%)\n",
      "Category B: 3 times (15.8%)\n",
      "\n",
      "Total Transmission Count by Category:\n",
      "Category A: 806 times (97.3%)\n",
      "Category B: 22 times (2.7%)\n",
      "\n",
      "--- Phase 1 Dynamic Penalty Statistics ---\n",
      "Average Penalty: 0.3374\n",
      "Maximum Penalty: 0.3666\n",
      "Minimum Penalty: 0.1000\n",
      "Average Nodes Polled per Time Step: 0.16\n",
      "Maximum Nodes Polled at any Time Step: 10\n",
      "\n",
      "--- Phase 2 Dynamic Penalty Statistics ---\n",
      "Average Penalty: 0.3666\n",
      "Maximum Penalty: 0.3666\n",
      "Minimum Penalty: 0.3666\n",
      "Average Nodes Polled per Time Step: 0.00\n",
      "Maximum Nodes Polled at any Time Step: 1\n",
      "\n",
      "--- Overall Dynamic Penalty Statistics ---\n",
      "Average Penalty: 0.3519\n",
      "Maximum Penalty: 0.3666\n",
      "Minimum Penalty: 0.0000\n",
      "Average Nodes Polled per Time Step: 0.08\n",
      "Maximum Nodes Polled at any Time Step: 10\n",
      "Target Maximum (M): 2\n",
      "\n",
      "--- Phase Shift Analysis ---\n",
      "In Phase 1, Category A was polled 41.58x more than Category B\n",
      "In Phase 2, Category B was polled 0.19x more than Category A\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset and preprocess\n",
    "#pivot_df = pd.read_csv(\"synthetic_scenario_30_nodes.csv\")\n",
    "pivot_df = pd.read_csv(\"synthetic_temp_polling_data_reversed.csv\")\n",
    "#pivot_df = pd.read_csv(\"synthetic_temp_polling_data.csv\")\n",
    "\n",
    "pivot_df = pivot_df.apply(lambda x: x.fillna(x.mean()), axis=0)  # Fill missing values\n",
    "pivot_df = pivot_df.head(10000)  # Restrict to 10000 time steps\n",
    "\n",
    "# Parameters\n",
    "reward = 0.5\n",
    "M = 2 # Maximum number of nodes that can be polled\n",
    "theta = 0.5  # Threshold for reward condition\n",
    "penalty = -0.5  # Penalty for polling when difference is <= theta\n",
    "initial_value = 20  # Initial estimate for last polled values\n",
    "\n",
    "# Set parameters\n",
    "beta_1 = 0.9  # dEWMA parameter for state value\n",
    "beta_2 = 0.9 # dEWMA parameter for rate of change\n",
    "\n",
    "# Extract column names, ensuring \"SN\" is excluded\n",
    "columns = [col for col in pivot_df.columns if col != \"SN\"]\n",
    "num_nodes = len(columns)  # Number of sensor nodes based on dataset columns\n",
    "num_time_steps = len(pivot_df)  # Total time steps based on dataset length\n",
    "\n",
    "# Track the number of times each category is pulled in each phase\n",
    "phase1_counts = {'Category A': 0, 'Category B': 0}\n",
    "phase2_counts = {'Category A': 0, 'Category B': 0}\n",
    "total_counts = {'Category A': 0, 'Category B': 0}\n",
    "\n",
    "# Function to calculate Age of Incorrect Information (AoII) at the sink\n",
    "def calculate_aoii_sink(current_time, last_received_time, last_rate_of_change):\n",
    "    # Handle potential NaN or Inf values\n",
    "    time_diff = current_time - last_received_time\n",
    "    if np.isnan(last_rate_of_change) or np.isinf(last_rate_of_change):\n",
    "        return 0.0  # Default to zero if rate of change is invalid\n",
    "    return abs(time_diff * last_rate_of_change)\n",
    "\n",
    "# Helper function to update state using dEWMA\n",
    "def update_node_state_dewma(measured_value, last_state_value, last_rate_of_change, delta_t, beta_1, beta_2):\n",
    "    # Handle the case where delta_t is 0 to avoid division by zero\n",
    "    if delta_t == 0:\n",
    "        return measured_value, last_rate_of_change  # Return measured value and keep the last rate of change\n",
    "    \n",
    "    x1 = beta_1 * measured_value + (1 - beta_1) * (last_state_value + last_rate_of_change * delta_t)\n",
    "    x2 = beta_2 * (x1 - last_state_value) / delta_t + (1 - beta_2) * last_rate_of_change\n",
    "    return x1, x2\n",
    "\n",
    "# Helper function to calculate reward\n",
    "def calculate_reward(measured_value, last_state_value, theta, penalty):\n",
    "    if abs(measured_value - last_state_value) > theta:\n",
    "        return reward  # Reward\n",
    "    return penalty  # Penalty\n",
    "\n",
    "# Function to extract numeric node ID from column names dynamically, ensuring valid extraction\n",
    "def extract_node_id(col_name):\n",
    "    \"\"\"Extract numeric node ID from column name, handling cases where no digits are found.\"\"\"\n",
    "    digits = ''.join(filter(str.isdigit, col_name))\n",
    "    return int(digits) if digits else None  # Return None if no digits are found\n",
    "\n",
    "# Function to get node category\n",
    "def get_node_category(node_id):\n",
    "    if node_id is None:\n",
    "        return None\n",
    "    if 1 <= node_id <= 5:\n",
    "        return 'Category A'\n",
    "    elif 6 <= node_id <= 10:\n",
    "        return 'Category B'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Dynamic Penalty Update algorithm based on Whittle indices\n",
    "def dynamic_penalty_update(whittle_indices, M, current_lambda):\n",
    "    # Convert dictionary values to a list of whittle indices, handling any NaN values\n",
    "    c_values = []\n",
    "    for v in whittle_indices.values():\n",
    "        if np.isnan(v):\n",
    "            c_values.append(-float('inf'))  # Treat NaN as negative infinity (won't be polled)\n",
    "        else:\n",
    "            c_values.append(v)\n",
    "    \n",
    "    # Identify the set ℰ of nodes where c_i > λ(t)\n",
    "    eligible_nodes = [i for i, c_i in enumerate(c_values) if c_i > current_lambda]\n",
    "    \n",
    "    # If |ℰ| ≤ M, no penalty update needed\n",
    "    if len(eligible_nodes) <= M:\n",
    "        return current_lambda\n",
    "    \n",
    "    # Sort the c_i values in descending order\n",
    "    sorted_c_values = sorted(c_values, reverse=True)\n",
    "    \n",
    "    # Identify the M-th value\n",
    "    M_th_value = sorted_c_values[M-1]\n",
    "    \n",
    "    # Update penalty to the M-th value\n",
    "    new_lambda = M_th_value\n",
    "    \n",
    "    return new_lambda\n",
    "\n",
    "# Main function to simulate Whittle AoII with dynamic penalty and track transmission counts by phase\n",
    "def run_simulation_whittle_aoii_dynamic_penalty(pivot_df, columns, M, theta, penalty):\n",
    "    cumulative_reward = 0  # Track total cumulative reward\n",
    "    cumulative_rewards = []  # Store cumulative average reward over time\n",
    "    last_update_times = {col: 0 for col in columns}  # Last update time for each node\n",
    "    state_node = {col: np.array([20.0, 0.1]) for col in columns}  # Node states\n",
    "    \n",
    "    # Initialize dynamic penalty (λ) to 0\n",
    "    aoii_penalty = 0.0\n",
    "    \n",
    "    # Track penalty evolution\n",
    "    penalty_values = [aoii_penalty]\n",
    "    \n",
    "    # Track nodes polled at each time step\n",
    "    nodes_polled_count = []\n",
    "    \n",
    "    # Track penalty values by phase\n",
    "    phase1_penalty_values = []\n",
    "    phase2_penalty_values = []\n",
    "    \n",
    "    # Track nodes polled by phase\n",
    "    phase1_nodes_polled = []\n",
    "    phase2_nodes_polled = []\n",
    "    \n",
    "    # Set minimum timestamp difference to avoid division by zero\n",
    "    min_delta_t = 1  # Minimum time difference of 1 to avoid division by zero\n",
    "\n",
    "    for t in range(len(pivot_df)):\n",
    "        # Determine current phase\n",
    "        current_phase = 1 if t < 5000 else 2\n",
    "        \n",
    "        # Step 1: Compute Whittle indices for each node based on AoII\n",
    "        whittle_indices = {}\n",
    "        for col in columns:\n",
    "            last_state_value, last_rate_of_change = state_node[col]\n",
    "            measured_value = pivot_df.loc[t, col]\n",
    "\n",
    "            # Correct AoII calculation at the sink using rate of change\n",
    "            current_aoii = calculate_aoii_sink(t, last_update_times[col], last_rate_of_change)\n",
    "            future_aoii_passive = calculate_aoii_sink(t+1, last_update_times[col], last_rate_of_change)\n",
    "            future_aoii_active = 0  # AoII resets to 0 if polled\n",
    "\n",
    "            # Whittle index calculations - with safety checks for NaN/Inf values\n",
    "            q_passive = current_aoii + future_aoii_passive\n",
    "            q_active = current_aoii + future_aoii_active + aoii_penalty\n",
    "            \n",
    "            # Calculate the Whittle index with safeguards against invalid values\n",
    "            whittle_index = q_passive - q_active\n",
    "            \n",
    "            # Handle NaN or Inf values\n",
    "            if np.isnan(whittle_index) or np.isinf(whittle_index):\n",
    "                whittle_indices[col] = -float('inf')  # Set to negative infinity if invalid\n",
    "            else:\n",
    "                whittle_indices[col] = whittle_index\n",
    "\n",
    "        # Step 2: Update the dynamic penalty (λ) using the algorithm\n",
    "        aoii_penalty = dynamic_penalty_update(whittle_indices, M, aoii_penalty)\n",
    "        penalty_values.append(aoii_penalty)\n",
    "        \n",
    "        # Track penalty by phase\n",
    "        if current_phase == 1:\n",
    "            phase1_penalty_values.append(aoii_penalty)\n",
    "        else:\n",
    "            phase2_penalty_values.append(aoii_penalty)\n",
    "        \n",
    "        # Step 3: Select nodes to poll based on the updated penalty\n",
    "        nodes_to_poll = [col for col in whittle_indices if whittle_indices[col] >= aoii_penalty]\n",
    "        nodes_polled = len(nodes_to_poll)\n",
    "        nodes_polled_count.append(nodes_polled)\n",
    "        \n",
    "        # Track nodes polled by phase\n",
    "        if current_phase == 1:\n",
    "            phase1_nodes_polled.append(nodes_polled)\n",
    "        else:\n",
    "            phase2_nodes_polled.append(nodes_polled)\n",
    "\n",
    "        # Step 4: Poll selected nodes and calculate rewards\n",
    "        for col in nodes_to_poll:\n",
    "            measured_value = pivot_df.loc[t, col]\n",
    "            last_state_value, last_rate_of_change = state_node[col]\n",
    "\n",
    "            # Calculate reward after polling\n",
    "            reward = calculate_reward(measured_value, last_state_value, theta, penalty)\n",
    "            cumulative_reward += reward  # Update cumulative reward\n",
    "\n",
    "            delta_t_dynamic = max(min_delta_t, t - last_update_times[col])  # Time since last update (ensure at least 1)\n",
    "\n",
    "            # Update node state and last update time\n",
    "            state_node[col] = update_node_state_dewma(\n",
    "                measured_value, last_state_value, last_rate_of_change, delta_t_dynamic, beta_1=beta_1, beta_2=beta_2\n",
    "            )\n",
    "            last_update_times[col] = t\n",
    "\n",
    "            # Extract node ID dynamically and categorize\n",
    "            node_id = extract_node_id(col)\n",
    "            if node_id is not None:\n",
    "                category = get_node_category(node_id)\n",
    "                if category:\n",
    "                    # Increment appropriate counter based on phase\n",
    "                    if current_phase == 1:\n",
    "                        phase1_counts[category] += 1\n",
    "                    else:\n",
    "                        phase2_counts[category] += 1\n",
    "                    \n",
    "                    # Increment total counter\n",
    "                    total_counts[category] += 1\n",
    "\n",
    "        # Step 5: Calculate cumulative average reward\n",
    "        cumulative_rewards.append(cumulative_reward / (t + 1))\n",
    "\n",
    "    # Compile phase-specific statistics\n",
    "    phase_stats = {\n",
    "        'phase1': {\n",
    "            'avg_penalty': np.mean(phase1_penalty_values) if phase1_penalty_values else 0,\n",
    "            'max_penalty': np.max(phase1_penalty_values) if phase1_penalty_values else 0,\n",
    "            'min_penalty': np.min(phase1_penalty_values) if phase1_penalty_values else 0,\n",
    "            'avg_nodes_polled': np.mean(phase1_nodes_polled) if phase1_nodes_polled else 0,\n",
    "            'max_nodes_polled': np.max(phase1_nodes_polled) if phase1_nodes_polled else 0\n",
    "        },\n",
    "        'phase2': {\n",
    "            'avg_penalty': np.mean(phase2_penalty_values) if phase2_penalty_values else 0,\n",
    "            'max_penalty': np.max(phase2_penalty_values) if phase2_penalty_values else 0,\n",
    "            'min_penalty': np.min(phase2_penalty_values) if phase2_penalty_values else 0,\n",
    "            'avg_nodes_polled': np.mean(phase2_nodes_polled) if phase2_nodes_polled else 0,\n",
    "            'max_nodes_polled': np.max(phase2_nodes_polled) if phase2_nodes_polled else 0\n",
    "        },\n",
    "        'total': {\n",
    "            'avg_penalty': np.mean(penalty_values),\n",
    "            'max_penalty': np.max(penalty_values),\n",
    "            'min_penalty': np.min(penalty_values),\n",
    "            'avg_nodes_polled': np.mean(nodes_polled_count),\n",
    "            'max_nodes_polled': np.max(nodes_polled_count)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return cumulative_rewards, phase1_counts, phase2_counts, total_counts, phase_stats\n",
    "\n",
    "# Run the simulation with dynamic penalty\n",
    "cumulative_rewards_whittle, phase1_counts, phase2_counts, total_counts, phase_stats = run_simulation_whittle_aoii_dynamic_penalty(\n",
    "    pivot_df, columns, M, theta, penalty\n",
    ")\n",
    "\n",
    "# Print the number of times each category was pulled in each phase\n",
    "print(\"Phase 1 (t=1 to t=5000) Transmission Count by Category:\")\n",
    "total_phase1 = sum(phase1_counts.values())\n",
    "for category, count in phase1_counts.items():\n",
    "    print(f\"{category}: {count} times ({count/total_phase1*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nPhase 2 (t=5001 to t=10000) Transmission Count by Category:\")\n",
    "total_phase2 = sum(phase2_counts.values())\n",
    "for category, count in phase2_counts.items():\n",
    "    print(f\"{category}: {count} times ({count/total_phase2*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nTotal Transmission Count by Category:\")\n",
    "total_overall = sum(total_counts.values())\n",
    "for category, count in total_counts.items():\n",
    "    print(f\"{category}: {count} times ({count/total_overall*100:.1f}%)\")\n",
    "\n",
    "# Save cumulative rewards to CSV\n",
    "pd.DataFrame(cumulative_rewards_whittle, columns=[\"cumulative_reward\"]).to_csv(\n",
    "    \"cumulative_rewards_whittle_dynamic_penalty.csv\", index=False\n",
    ")\n",
    "\n",
    "# Print statistics for each phase\n",
    "print(\"\\n--- Phase 1 Dynamic Penalty Statistics ---\")\n",
    "print(f\"Average Penalty: {phase_stats['phase1']['avg_penalty']:.4f}\")\n",
    "print(f\"Maximum Penalty: {phase_stats['phase1']['max_penalty']:.4f}\")\n",
    "print(f\"Minimum Penalty: {phase_stats['phase1']['min_penalty']:.4f}\")\n",
    "print(f\"Average Nodes Polled per Time Step: {phase_stats['phase1']['avg_nodes_polled']:.2f}\")\n",
    "print(f\"Maximum Nodes Polled at any Time Step: {phase_stats['phase1']['max_nodes_polled']}\")\n",
    "\n",
    "print(\"\\n--- Phase 2 Dynamic Penalty Statistics ---\")\n",
    "print(f\"Average Penalty: {phase_stats['phase2']['avg_penalty']:.4f}\")\n",
    "print(f\"Maximum Penalty: {phase_stats['phase2']['max_penalty']:.4f}\")\n",
    "print(f\"Minimum Penalty: {phase_stats['phase2']['min_penalty']:.4f}\")\n",
    "print(f\"Average Nodes Polled per Time Step: {phase_stats['phase2']['avg_nodes_polled']:.2f}\")\n",
    "print(f\"Maximum Nodes Polled at any Time Step: {phase_stats['phase2']['max_nodes_polled']}\")\n",
    "\n",
    "print(\"\\n--- Overall Dynamic Penalty Statistics ---\")\n",
    "print(f\"Average Penalty: {phase_stats['total']['avg_penalty']:.4f}\")\n",
    "print(f\"Maximum Penalty: {phase_stats['total']['max_penalty']:.4f}\")\n",
    "print(f\"Minimum Penalty: {phase_stats['total']['min_penalty']:.4f}\")\n",
    "print(f\"Average Nodes Polled per Time Step: {phase_stats['total']['avg_nodes_polled']:.2f}\")\n",
    "print(f\"Maximum Nodes Polled at any Time Step: {phase_stats['total']['max_nodes_polled']}\")\n",
    "print(f\"Target Maximum (M): {M}\")\n",
    "\n",
    "# Print summary of the phase shift\n",
    "print(\"\\n--- Phase Shift Analysis ---\")\n",
    "if phase1_counts['Category B'] > 0 and phase2_counts['Category A'] > 0:\n",
    "    print(f\"In Phase 1, Category A was polled {phase1_counts['Category A']/phase1_counts['Category B']:.2f}x more than Category B\")\n",
    "    print(f\"In Phase 2, Category B was polled {phase2_counts['Category B']/phase2_counts['Category A']:.2f}x more than Category A\")\n",
    "else:\n",
    "    print(\"Unable to calculate polling ratios (division by zero)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transmission Count by Category:\n",
      "Category A: 604 times\n",
      "Category B: 18 times\n",
      "\n",
      "Dynamic Penalty Statistics:\n",
      "Average Penalty: 0.5225\n",
      "Maximum Penalty: 0.6829\n",
      "Minimum Penalty: 0.0000\n",
      "\n",
      "Nodes Polled Statistics:\n",
      "Average Nodes Polled per Time Step: 0.06\n",
      "Maximum Nodes Polled at any Time Step: 10\n",
      "Target Maximum (M): 1\n",
      "\n",
      "Total MSE: 10.957372\n",
      "\n",
      "MSE by Category:\n",
      "Category A: 21.725755\n",
      "Category B: 0.188989\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load dataset and preprocess\n",
    "#pivot_df = pd.read_csv(\"synthetic_scenario_30_nodes.csv\")\n",
    "#pivot_df = pd.read_csv(\"simulated_office_environment.csv\")\n",
    "pivot_df = pd.read_csv(\"synthetic_temp_polling_data.csv\")\n",
    "\n",
    "pivot_df = pivot_df.apply(lambda x: x.fillna(x.mean()), axis=0)  # Fill missing values\n",
    "pivot_df = pivot_df.head(10000)  # Restrict to first 10000 time steps\n",
    "\n",
    "# Parameters\n",
    "reward = 0.5\n",
    "M = 1 # Maximum number of nodes that can be polled\n",
    "theta = 0.5  # Threshold for reward condition\n",
    "penalty = -0.5  # Penalty for polling when difference is <= theta\n",
    "initial_value = 20  # Initial estimate for last polled values\n",
    "\n",
    "# Set parameters\n",
    "beta_1 = 0.99  # dEWMA parameter for state value\n",
    "beta_2 = 0.99 # dEWMA parameter for rate of change\n",
    "\n",
    "# Extract column names, ensuring \"SN\" is excluded\n",
    "columns = [col for col in pivot_df.columns if col != \"SN\"]\n",
    "num_nodes = len(columns)  # Number of sensor nodes based on dataset columns\n",
    "num_time_steps = len(pivot_df)  # Total time steps based on dataset length\n",
    "\n",
    "# Track the number of times each category is pulled\n",
    "category_counts = {'Category A': 0, 'Category B': 0}\n",
    "\n",
    "# Function to calculate Age of Incorrect Information (AoII) at the sink\n",
    "def calculate_aoii_sink(current_time, last_received_time, last_rate_of_change):\n",
    "    # Handle potential NaN or Inf values\n",
    "    time_diff = current_time - last_received_time\n",
    "    if np.isnan(last_rate_of_change) or np.isinf(last_rate_of_change):\n",
    "        return 0.0  # Default to zero if rate of change is invalid\n",
    "    return abs(time_diff * last_rate_of_change)\n",
    "\n",
    "# Helper function to update state using dEWMA\n",
    "def update_node_state_dewma(measured_value, last_state_value, last_rate_of_change, delta_t, beta_1, beta_2):\n",
    "    # Handle the case where delta_t is 0 to avoid division by zero\n",
    "    if delta_t == 0:\n",
    "        return measured_value, last_rate_of_change  # Return measured value and keep the last rate of change\n",
    "    \n",
    "    x1 = beta_1 * measured_value + (1 - beta_1) * (last_state_value + last_rate_of_change * delta_t)\n",
    "    x2 = beta_2 * (x1 - last_state_value) / delta_t + (1 - beta_2) * last_rate_of_change\n",
    "    return x1, x2\n",
    "\n",
    "# Helper function to calculate reward\n",
    "def calculate_reward(measured_value, last_state_value, theta, penalty):\n",
    "    if abs(measured_value - last_state_value) > theta:\n",
    "        return reward  # Reward\n",
    "    return penalty  # Penalty\n",
    "\n",
    "# Function to extract numeric node ID from column names dynamically, ensuring valid extraction\n",
    "def extract_node_id(col_name):\n",
    "    \"\"\"Extract numeric node ID from column name, handling cases where no digits are found.\"\"\"\n",
    "    digits = ''.join(filter(str.isdigit, col_name))\n",
    "    return int(digits) if digits else None  # Return None if no digits are found\n",
    "\n",
    "# Function to get node category\n",
    "def get_node_category(node_id):\n",
    "    if node_id is None:\n",
    "        return None\n",
    "    if 1 <= node_id <= 5:\n",
    "        return 'Category A'\n",
    "    elif 6 <= node_id <= 10:\n",
    "        return 'Category B'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Dynamic Penalty Update algorithm based on Whittle indices\n",
    "def dynamic_penalty_update(whittle_indices, M, current_lambda):\n",
    "    # Convert dictionary values to a list of whittle indices, handling any NaN values\n",
    "    c_values = []\n",
    "    for v in whittle_indices.values():\n",
    "        if np.isnan(v):\n",
    "            c_values.append(-float('inf'))  # Treat NaN as negative infinity (won't be polled)\n",
    "        else:\n",
    "            c_values.append(v)\n",
    "    \n",
    "    # Identify the set ℰ of nodes where c_i > λ(t)\n",
    "    eligible_nodes = [i for i, c_i in enumerate(c_values) if c_i > current_lambda]\n",
    "    \n",
    "    # If |ℰ| ≤ M, no penalty update needed\n",
    "    if len(eligible_nodes) <= M:\n",
    "        return current_lambda\n",
    "    \n",
    "    # Sort the c_i values in descending order\n",
    "    sorted_c_values = sorted(c_values, reverse=True)\n",
    "    \n",
    "    # Identify the M-th value\n",
    "    M_th_value = sorted_c_values[M-1]\n",
    "    \n",
    "    # Update penalty to the M-th value\n",
    "    new_lambda = M_th_value\n",
    "    \n",
    "    return new_lambda\n",
    "\n",
    "# Main function to simulate Whittle AoII with rewards and track transmission counts\n",
    "def run_simulation_whittle_aoii_dynamic_penalty(pivot_df, columns, M, theta, penalty):\n",
    "    cumulative_reward = 0  # Track total cumulative reward\n",
    "    cumulative_rewards = []  # Store cumulative average reward over time\n",
    "    last_update_times = {col: 0 for col in columns}  # Last update time for each node\n",
    "    state_node = {col: np.array([20.0, 0.1]) for col in columns}  # Node states\n",
    "    \n",
    "    # Initialize dynamic penalty (λ) to 0\n",
    "    aoii_penalty = 0.0\n",
    "    \n",
    "    # Track penalty evolution\n",
    "    penalty_values = [aoii_penalty]\n",
    "    \n",
    "    # Track nodes polled at each time step\n",
    "    nodes_polled_count = []\n",
    "    \n",
    "    # Set minimum timestamp difference to avoid division by zero\n",
    "    min_delta_t = 1  # Minimum time difference of 1 to avoid division by zero\n",
    "\n",
    "    # Initialize storage for MSE calculation\n",
    "    true_values = {col: [] for col in columns}\n",
    "    predicted_values = {col: [] for col in columns}\n",
    "\n",
    "    for t in range(len(pivot_df)):\n",
    "        # Step 1: Compute Whittle indices for each node based on AoII\n",
    "        whittle_indices = {}\n",
    "        for col in columns:\n",
    "            last_state_value, last_rate_of_change = state_node[col]\n",
    "            measured_value = pivot_df.loc[t, col]\n",
    "\n",
    "            # Store true value for all nodes at each time step\n",
    "            true_values[col].append(measured_value)\n",
    "            \n",
    "            # Generate prediction for current time using the last known state\n",
    "            delta_t_for_prediction = max(min_delta_t, t - last_update_times[col])\n",
    "            predicted_value = last_state_value + last_rate_of_change * delta_t_for_prediction\n",
    "            predicted_values[col].append(predicted_value)\n",
    "\n",
    "            # Correct AoII calculation at the sink using rate of change\n",
    "            current_aoii = calculate_aoii_sink(t, last_update_times[col], last_rate_of_change)\n",
    "            future_aoii_passive = calculate_aoii_sink(t+1, last_update_times[col], last_rate_of_change)\n",
    "            future_aoii_active = 0  # AoII resets to 0 if polled\n",
    "\n",
    "            # Whittle index calculations - with safety checks for NaN/Inf values\n",
    "            q_passive = current_aoii + future_aoii_passive\n",
    "            q_active = current_aoii + future_aoii_active + aoii_penalty\n",
    "            \n",
    "            # Calculate the Whittle index with safeguards against invalid values\n",
    "            whittle_index = q_passive - q_active\n",
    "            \n",
    "            # Handle NaN or Inf values\n",
    "            if np.isnan(whittle_index) or np.isinf(whittle_index):\n",
    "                whittle_indices[col] = -float('inf')  # Set to negative infinity if invalid\n",
    "            else:\n",
    "                whittle_indices[col] = whittle_index\n",
    "\n",
    "        # Step 2: Update the dynamic penalty (λ) using the algorithm\n",
    "        aoii_penalty = dynamic_penalty_update(whittle_indices, M, aoii_penalty)\n",
    "        penalty_values.append(aoii_penalty)\n",
    "        \n",
    "        # Step 3: Select nodes to poll based on the updated penalty\n",
    "        nodes_to_poll = [col for col in whittle_indices if whittle_indices[col] >= aoii_penalty]\n",
    "        nodes_polled_count.append(len(nodes_to_poll))\n",
    "\n",
    "        # Step 4: Poll selected nodes and calculate rewards\n",
    "        for col in nodes_to_poll:\n",
    "            measured_value = pivot_df.loc[t, col]\n",
    "            last_state_value, last_rate_of_change = state_node[col]\n",
    "\n",
    "            # Calculate reward after polling\n",
    "            reward = calculate_reward(measured_value, last_state_value, theta, penalty)\n",
    "            cumulative_reward += reward  # Update cumulative reward\n",
    "\n",
    "            delta_t_dynamic = max(min_delta_t, t - last_update_times[col])  # Time since last update (ensure at least 1)\n",
    "\n",
    "            # Update node state and last update time\n",
    "            state_node[col] = update_node_state_dewma(\n",
    "                measured_value, last_state_value, last_rate_of_change, delta_t_dynamic, beta_1=beta_1, beta_2=beta_2\n",
    "            )\n",
    "            last_update_times[col] = t\n",
    "\n",
    "            # Extract node ID dynamically and categorize\n",
    "            node_id = extract_node_id(col)\n",
    "            if node_id is not None:\n",
    "                if 1 <= node_id <= 5:\n",
    "                    category_counts['Category A'] += 1\n",
    "                elif 6 <= node_id <= 10:\n",
    "                    category_counts['Category B'] += 1\n",
    "             \n",
    "        # Step 5: Calculate cumulative average reward\n",
    "        cumulative_rewards.append(cumulative_reward / (t + 1))\n",
    "\n",
    "    return cumulative_rewards, category_counts, penalty_values, nodes_polled_count, true_values, predicted_values\n",
    "\n",
    "# Run the simulation with dynamic penalty\n",
    "cumulative_rewards_whittle, category_pulled_counts, penalty_values, nodes_polled_count, true_values, predicted_values = run_simulation_whittle_aoii_dynamic_penalty(\n",
    "    pivot_df, columns, M, theta, penalty\n",
    ")\n",
    "\n",
    "# Print the number of times each category was pulled\n",
    "print(\"Transmission Count by Category:\")\n",
    "for category, count in category_pulled_counts.items():\n",
    "    print(f\"{category}: {count} times\")\n",
    "\n",
    "# Save cumulative rewards to CSV\n",
    "pd.DataFrame(cumulative_rewards_whittle, columns=[\"cumulative_reward\"]).to_csv(\n",
    "    \"cumulative_rewards_whittle_dynamic_penalty.csv\", index=False\n",
    ")\n",
    "\n",
    "# Calculate statistics about the penalty values and nodes polled\n",
    "avg_penalty = np.mean(penalty_values)\n",
    "max_penalty = np.max(penalty_values)\n",
    "min_penalty = np.min(penalty_values)\n",
    "avg_nodes_polled = np.mean(nodes_polled_count)\n",
    "max_nodes_polled = np.max(nodes_polled_count)\n",
    "\n",
    "print(f\"\\nDynamic Penalty Statistics:\")\n",
    "print(f\"Average Penalty: {avg_penalty:.4f}\")\n",
    "print(f\"Maximum Penalty: {max_penalty:.4f}\")\n",
    "print(f\"Minimum Penalty: {min_penalty:.4f}\")\n",
    "print(f\"\\nNodes Polled Statistics:\")\n",
    "print(f\"Average Nodes Polled per Time Step: {avg_nodes_polled:.2f}\")\n",
    "print(f\"Maximum Nodes Polled at any Time Step: {max_nodes_polled}\")\n",
    "print(f\"Target Maximum (M): {M}\")\n",
    "\n",
    "# Calculate MSE for each node\n",
    "node_mse = {}\n",
    "for col in columns:\n",
    "    # Handle potential NaN values\n",
    "    pred_array = np.array(predicted_values[col])\n",
    "    true_array = np.array(true_values[col])\n",
    "    \n",
    "    # Create mask for non-NaN values\n",
    "    valid_mask = ~np.isnan(pred_array) & ~np.isnan(true_array)\n",
    "    \n",
    "    # Only calculate MSE with valid values\n",
    "    if np.any(valid_mask):\n",
    "        node_mse[col] = mean_squared_error(\n",
    "            true_array[valid_mask], \n",
    "            pred_array[valid_mask]\n",
    "        )\n",
    "    else:\n",
    "        node_mse[col] = np.nan\n",
    "\n",
    "# Calculate total MSE across all nodes\n",
    "all_true_values = []\n",
    "all_predicted_values = []\n",
    "\n",
    "for col in columns:\n",
    "    # Get arrays for this column\n",
    "    pred_array = np.array(predicted_values[col])\n",
    "    true_array = np.array(true_values[col])\n",
    "    \n",
    "    # Filter out NaN values\n",
    "    valid_mask = ~np.isnan(pred_array) & ~np.isnan(true_array)\n",
    "    \n",
    "    # Add valid values to the combined arrays\n",
    "    all_true_values.extend(true_array[valid_mask])\n",
    "    all_predicted_values.extend(pred_array[valid_mask])\n",
    "\n",
    "# Calculate overall MSE if we have valid data\n",
    "if all_true_values and all_predicted_values:\n",
    "    total_mse = mean_squared_error(all_true_values, all_predicted_values)\n",
    "    print(f\"\\nTotal MSE: {total_mse:.6f}\")\n",
    "else:\n",
    "    print(\"\\nUnable to calculate Total MSE: No valid data points\")\n",
    "\n",
    "# Calculate MSE per category\n",
    "category_mse = {'Category A': [], 'Category B': []}\n",
    "for col in columns:\n",
    "    node_id = extract_node_id(col)\n",
    "    category = get_node_category(node_id)\n",
    "    if category and not np.isnan(node_mse[col]):\n",
    "        category_mse[category].append(node_mse[col])\n",
    "\n",
    "# Print MSE per category\n",
    "print(\"\\nMSE by Category:\")\n",
    "for category, mse_values in category_mse.items():\n",
    "    if mse_values:\n",
    "        avg_mse = np.mean(mse_values)\n",
    "        print(f\"{category}: {avg_mse:.6f}\")\n",
    "    else:\n",
    "        print(f\"{category}: No valid data for MSE calculation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transmission Count by Category:\n",
      "Category A: 1139 times\n",
      "Category B: 21 times\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#pivot_df = pd.read_csv(\"synthetic_scenario_30_nodes.csv\")\n",
    "\n",
    "pivot_df = pd.read_csv(\"synthetic_temp_polling_data_reversed.csv\")\n",
    "#pivot_df = pd.read_csv(\"synthetic_temp_polling_data.csv\")\n",
    "pivot_df = pivot_df.head(10000)  # Restrict to first 20000 time steps\n",
    "\n",
    "\n",
    "M = 1  # Maximum number of nodes that can be polled\n",
    "aoii_penalty = 0.5\n",
    "initial_value = 20  # Initial estimate for last polled values\n",
    "\n",
    "# Set parameters\n",
    "beta_1 = 0.9  # dEWMA parameter for state value\n",
    "beta_2 = 0.9 # dEWMA parameter for rate of change\n",
    "\n",
    "# Extract column names, ensuring \"SN\" is excluded\n",
    "columns = [col for col in pivot_df.columns if col != \"SN\"]\n",
    "num_nodes = len(columns)  # Number of sensor nodes based on dataset columns\n",
    "num_time_steps = len(pivot_df)  # Total time steps based on dataset length\n",
    "\n",
    "# Track the number of times each category is pulled\n",
    "category_counts = {'Category A': 0, 'Category B': 0}\n",
    "\n",
    "# Function to calculate Age of Incorrect Information (AoII) at the sink\n",
    "def calculate_aoii_sink(current_time, last_received_time, last_rate_of_change):\n",
    "    return abs((current_time - last_received_time) * last_rate_of_change)\n",
    "\n",
    "# Helper function to update state using dEWMA\n",
    "def update_node_state_dewma(measured_value, last_state_value, last_rate_of_change, delta_t, beta_1, beta_2):\n",
    "    x1 = beta_1 * measured_value + (1 - beta_1) * (last_state_value + last_rate_of_change * delta_t)\n",
    "    x2 = beta_2 * (x1 - last_state_value) / delta_t + (1 - beta_2) * last_rate_of_change\n",
    "    return x1, x2\n",
    "\n",
    "\n",
    "# Function to extract numeric node ID from column names dynamically, ensuring valid extraction\n",
    "def extract_node_id(col_name):\n",
    "    \"\"\"Extract numeric node ID from column name, handling cases where no digits are found.\"\"\"\n",
    "    digits = ''.join(filter(str.isdigit, col_name))\n",
    "    return int(digits) if digits else None  # Return None if no digits are found\n",
    "\n",
    "# Main function to simulate Whittle AoII with rewards and track transmission counts\n",
    "def run_simulation_whittle_aoii(pivot_df, columns, M, aoii_penalty):\n",
    "    \n",
    "    last_update_times = {col: 0 for col in columns}  # Last update time for each node\n",
    "    state_node = {col: np.array([20.0, 0.1]) for col in columns}  # Node states\n",
    "\n",
    "    for t in range(len(pivot_df)):\n",
    "        # Step 1: Compute Whittle indices for each node based on AoII\n",
    "        whittle_indices = {}\n",
    "        for col in columns:\n",
    "            last_state_value, last_rate_of_change = state_node[col]\n",
    "            measured_value = pivot_df.loc[t, col]\n",
    "\n",
    "            # Correct AoII calculation at the sink using rate of change\n",
    "            current_aoii = calculate_aoii_sink(t, last_update_times[col], last_rate_of_change)\n",
    "            future_aoii_passive = calculate_aoii_sink(t+1, last_update_times[col], last_rate_of_change)\n",
    "            future_aoii_active = 0  # AoII resets to 0 if polled\n",
    "\n",
    "            # Whittle index calculations\n",
    "            q_passive = current_aoii + future_aoii_passive\n",
    "            q_active = current_aoii + future_aoii_active + aoii_penalty\n",
    "            whittle_indices[col] = q_passive - q_active\n",
    "\n",
    "        # Step 2: Select top M nodes to poll based on Whittle indices\n",
    "        nodes_to_poll = [col for col in whittle_indices if whittle_indices[col] >= 0]\n",
    "        if len(nodes_to_poll) > M:\n",
    "            nodes_to_poll = sorted(nodes_to_poll, key=whittle_indices.get, reverse=True)[:M]\n",
    "\n",
    "        # Step 3: Poll selected nodes and calculate rewards\n",
    "        for col in nodes_to_poll:\n",
    "            measured_value = pivot_df.loc[t, col]\n",
    "            last_state_value, last_rate_of_change = state_node[col]\n",
    "\n",
    "            delta_t_dynamic = t - last_update_times[col]  # Time since last update\n",
    "\n",
    "            # Update node state and last update time\n",
    "            state_node[col] = update_node_state_dewma(\n",
    "                measured_value, last_state_value, last_rate_of_change, delta_t_dynamic, beta_1=beta_1, beta_2=beta_2\n",
    "            )\n",
    "            last_update_times[col] = t\n",
    "\n",
    "            # Extract node ID dynamically and categorize\n",
    "            node_id = extract_node_id(col)\n",
    "            if node_id is not None:\n",
    "                if 1 <= node_id <= 5:\n",
    "                    category_counts['Category A'] += 1\n",
    "                elif 6 <= node_id <= 10:\n",
    "                    category_counts['Category B'] += 1\n",
    "\n",
    "\n",
    "    return  category_counts\n",
    "\n",
    "# Run the simulation\n",
    "category_pulled_counts = run_simulation_whittle_aoii(\n",
    "    pivot_df, columns, M, aoii_penalty\n",
    ")\n",
    "\n",
    "# Print the number of times each category was pulled\n",
    "print(\"Transmission Count by Category:\")\n",
    "for category, count in category_pulled_counts.items():\n",
    "    print(f\"{category}: {count} times\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1 (t=1 to t=5000) Transmission Count by Category:\n",
      "Category A: 594 times (67.0%)\n",
      "Category B: 293 times (33.0%)\n",
      "\n",
      "Phase 2 (t=5001 to t=10000) Transmission Count by Category:\n",
      "Category A: 100 times (61.0%)\n",
      "Category B: 64 times (39.0%)\n",
      "\n",
      "Total Transmission Count by Category:\n",
      "Category A: 694 times (66.0%)\n",
      "Category B: 357 times (34.0%)\n",
      "\n",
      "--- Phase Shift Analysis ---\n",
      "In Phase 1, Category A was polled 2.03x more than Category B\n",
      "In Phase 2, Category B was polled 0.64x more than Category A\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "#pivot_df = pd.read_csv(\"synthetic_temp_polling_data_reversed.csv\")\n",
    "pivot_df = pd.read_csv(\"synthetic_temp_polling_data.csv\")\n",
    "pivot_df = pivot_df.head(10000)  # Restrict to 10000 time steps\n",
    "\n",
    "# Parameters\n",
    "M = 2  # Maximum number of nodes that can be polled\n",
    "aoii_penalty = 0.5\n",
    "initial_value = 20  # Initial estimate for last polled values\n",
    "\n",
    "# dEWMA parameters\n",
    "beta_1 = 0.8  # dEWMA parameter for state value\n",
    "beta_2 = 0.3  # dEWMA parameter for rate of change\n",
    "\n",
    "# Extract column names, ensuring \"SN\" is excluded\n",
    "columns = [col for col in pivot_df.columns if col != \"SN\"]\n",
    "\n",
    "# Track the number of times each category is pulled in each phase\n",
    "phase1_counts = {'Category A': 0, 'Category B': 0}\n",
    "phase2_counts = {'Category A': 0, 'Category B': 0}\n",
    "total_counts = {'Category A': 0, 'Category B': 0}\n",
    "\n",
    "# Function to calculate Age of Incorrect Information (AoII) at the sink\n",
    "def calculate_aoii_sink(current_time, last_received_time, last_rate_of_change):\n",
    "    return abs((current_time - last_received_time) * last_rate_of_change)\n",
    "\n",
    "# Helper function to update state using dEWMA\n",
    "def update_node_state_dewma(measured_value, last_state_value, last_rate_of_change, delta_t, beta_1, beta_2):\n",
    "    # Handle the case where delta_t is 0 to avoid division by zero\n",
    "    if delta_t == 0:\n",
    "        return measured_value, last_rate_of_change\n",
    "        \n",
    "    x1 = beta_1 * measured_value + (1 - beta_1) * (last_state_value + last_rate_of_change * delta_t)\n",
    "    x2 = beta_2 * (x1 - last_state_value) / delta_t + (1 - beta_2) * last_rate_of_change\n",
    "    return x1, x2\n",
    "\n",
    "# Function to extract node ID\n",
    "def extract_node_id(col_name):\n",
    "    digits = ''.join(filter(str.isdigit, col_name))\n",
    "    return int(digits) if digits else None\n",
    "\n",
    "# Function to get node category\n",
    "def get_node_category(node_id):\n",
    "    if node_id is None:\n",
    "        return None\n",
    "    if 1 <= node_id <= 5:\n",
    "        return 'Category A'\n",
    "    elif 6 <= node_id <= 10:\n",
    "        return 'Category B'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Main simulation function\n",
    "def run_simulation_whittle_aoii(pivot_df, columns, M, aoii_penalty):\n",
    "    last_update_times = {col: 0 for col in columns}  # Last update time for each node\n",
    "    state_node = {col: np.array([20.0, 0.1]) for col in columns}  # Node states\n",
    "    min_delta_t = 1  # Minimum time difference to avoid division by zero\n",
    "\n",
    "    for t in range(len(pivot_df)):\n",
    "        # Determine current phase (1: t < 5000, 2: t >= 5000)\n",
    "        current_phase = 1 if t < 5000 else 2\n",
    "        \n",
    "        # Step 1: Compute Whittle indices for each node based on AoII\n",
    "        whittle_indices = {}\n",
    "        for col in columns:\n",
    "            last_state_value, last_rate_of_change = state_node[col]\n",
    "\n",
    "            # Calculate AoII values\n",
    "            current_aoii = calculate_aoii_sink(t, last_update_times[col], last_rate_of_change)\n",
    "            future_aoii_passive = calculate_aoii_sink(t+1, last_update_times[col], last_rate_of_change)\n",
    "            future_aoii_active = 0  # AoII resets to 0 if polled\n",
    "\n",
    "            # Whittle index calculations\n",
    "            q_passive = current_aoii + future_aoii_passive\n",
    "            q_active = current_aoii + future_aoii_active + aoii_penalty\n",
    "            whittle_indices[col] = q_passive - q_active\n",
    "\n",
    "        # Step 2: Select nodes to poll based on Whittle indices\n",
    "        nodes_to_poll = [col for col in whittle_indices if whittle_indices[col] >= 0]\n",
    "        if len(nodes_to_poll) > M:\n",
    "            nodes_to_poll = sorted(nodes_to_poll, key=whittle_indices.get, reverse=True)[:M]\n",
    "\n",
    "        # Step 3: Poll selected nodes and update states\n",
    "        for col in nodes_to_poll:\n",
    "            measured_value = pivot_df.loc[t, col]\n",
    "            last_state_value, last_rate_of_change = state_node[col]\n",
    "\n",
    "            delta_t_dynamic = max(min_delta_t, t - last_update_times[col])  # Time since last update\n",
    "\n",
    "            # Update node state and last update time\n",
    "            state_node[col] = update_node_state_dewma(\n",
    "                measured_value, last_state_value, last_rate_of_change, delta_t_dynamic, beta_1, beta_2\n",
    "            )\n",
    "            last_update_times[col] = t\n",
    "\n",
    "            # Track category counts by phase\n",
    "            node_id = extract_node_id(col)\n",
    "            if node_id is not None:\n",
    "                category = get_node_category(node_id)\n",
    "                if category:\n",
    "                    # Update phase-specific counter\n",
    "                    if current_phase == 1:\n",
    "                        phase1_counts[category] += 1\n",
    "                    else:\n",
    "                        phase2_counts[category] += 1\n",
    "                    \n",
    "                    # Update total counter\n",
    "                    total_counts[category] += 1\n",
    "\n",
    "    return phase1_counts, phase2_counts, total_counts\n",
    "\n",
    "# Run the simulation\n",
    "phase1_counts, phase2_counts, total_counts = run_simulation_whittle_aoii(\n",
    "    pivot_df, columns, M, aoii_penalty\n",
    ")\n",
    "\n",
    "# Print the transmission counts by category for each phase\n",
    "print(\"Phase 1 (t=1 to t=5000) Transmission Count by Category:\")\n",
    "total_phase1 = sum(phase1_counts.values())\n",
    "for category, count in phase1_counts.items():\n",
    "    print(f\"{category}: {count} times ({count/total_phase1*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nPhase 2 (t=5001 to t=10000) Transmission Count by Category:\")\n",
    "total_phase2 = sum(phase2_counts.values())\n",
    "for category, count in phase2_counts.items():\n",
    "    print(f\"{category}: {count} times ({count/total_phase2*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nTotal Transmission Count by Category:\")\n",
    "total_overall = sum(total_counts.values())\n",
    "for category, count in total_counts.items():\n",
    "    print(f\"{category}: {count} times ({count/total_overall*100:.1f}%)\")\n",
    "\n",
    "# Print summary of the phase shift\n",
    "print(\"\\n--- Phase Shift Analysis ---\")\n",
    "print(f\"In Phase 1, Category A was polled {phase1_counts['Category A']/phase1_counts['Category B']:.2f}x more than Category B\")\n",
    "print(f\"In Phase 2, Category B was polled {phase2_counts['Category B']/phase2_counts['Category A']:.2f}x more than Category A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polling Distribution by Time Window:\n",
      "----------------------------------\n",
      "\n",
      "Window 1: t=1-1000\n",
      "Category A: 99 times (86.1%)\n",
      "Category B: 16 times (13.9%)\n",
      "Total Polled: 115\n",
      "Average Penalty: 0.5565\n",
      "Average Nodes Polled: 0.12\n",
      "\n",
      "Window 2: t=1001-2000\n",
      "Category A: 47 times (97.9%)\n",
      "Category B: 1 times (2.1%)\n",
      "Total Polled: 48\n",
      "Average Penalty: 0.6096\n",
      "Average Nodes Polled: 0.05\n",
      "\n",
      "Window 3: t=2001-3000\n",
      "Category A: 30 times (96.8%)\n",
      "Category B: 1 times (3.2%)\n",
      "Total Polled: 31\n",
      "Average Penalty: 0.6227\n",
      "Average Nodes Polled: 0.03\n",
      "\n",
      "Window 4: t=3001-4000\n",
      "Category A: 43 times (100.0%)\n",
      "Category B: 0 times (0.0%)\n",
      "Total Polled: 43\n",
      "Average Penalty: 0.6648\n",
      "Average Nodes Polled: 0.04\n",
      "\n",
      "Window 5: t=4001-5000\n",
      "Category A: 17 times (94.4%)\n",
      "Category B: 1 times (5.6%)\n",
      "Total Polled: 18\n",
      "Average Penalty: 0.6808\n",
      "Average Nodes Polled: 0.02\n",
      "\n",
      "Window 6: t=5001-6000\n",
      "Category A: 11 times (100.0%)\n",
      "Category B: 0 times (0.0%)\n",
      "Total Polled: 11\n",
      "Average Penalty: 0.6808\n",
      "Average Nodes Polled: 0.01\n",
      "\n",
      "Window 7: t=6001-7000\n",
      "Category A: 2 times (66.7%)\n",
      "Category B: 1 times (33.3%)\n",
      "Total Polled: 3\n",
      "Average Penalty: 0.6808\n",
      "Average Nodes Polled: 0.00\n",
      "\n",
      "Window 8: t=7001-8000\n",
      "Category A: 2 times (100.0%)\n",
      "Category B: 0 times (0.0%)\n",
      "Total Polled: 2\n",
      "Average Penalty: 0.6808\n",
      "Average Nodes Polled: 0.00\n",
      "\n",
      "Window 9: t=8001-9000\n",
      "Category A: 0 times (0.0%)\n",
      "Category B: 0 times (0.0%)\n",
      "Total Polled: 0\n",
      "Average Penalty: 0.6808\n",
      "Average Nodes Polled: 0.00\n",
      "\n",
      "Window 10: t=9001-10000\n",
      "Category A: 0 times (0.0%)\n",
      "Category B: 0 times (0.0%)\n",
      "Total Polled: 0\n",
      "Average Penalty: 0.6808\n",
      "Average Nodes Polled: 0.00\n",
      "\n",
      "Window-by-Window Summary (CSV format):\n",
      "Window,Category A Count,Category B Count,Category A %,Category B %,Total Polled,Avg Penalty,Avg Nodes Polled\n",
      "t=1-1000,99,16,86.1,13.9,115,0.5565,0.12\n",
      "t=1001-2000,47,1,97.9,2.1,48,0.6096,0.05\n",
      "t=2001-3000,30,1,96.8,3.2,31,0.6227,0.03\n",
      "t=3001-4000,43,0,100.0,0.0,43,0.6648,0.04\n",
      "t=4001-5000,17,1,94.4,5.6,18,0.6808,0.02\n",
      "t=5001-6000,11,0,100.0,0.0,11,0.6808,0.01\n",
      "t=6001-7000,2,1,66.7,33.3,3,0.6808,0.00\n",
      "t=7001-8000,2,0,100.0,0.0,2,0.6808,0.00\n",
      "t=8001-9000,0,0,0.0,0.0,0,0.6808,0.00\n",
      "t=9001-10000,0,0,0.0,0.0,0,0.6808,0.00\n",
      "\n",
      "Pre-Transition Summary (t=1-5000):\n",
      "Category A: 236 times (92.5%)\n",
      "Category B: 19 times (7.5%)\n",
      "\n",
      "Post-Transition Summary (t=5001-10000):\n",
      "Category A: 15 times (93.8%)\n",
      "Category B: 1 times (6.2%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset and preprocess\n",
    "#pivot_df = pd.read_csv(\"synthetic_temp_polling_data.csv\")\n",
    "pivot_df = pd.read_csv(\"synthetic_temp_polling_data_reversed.csv\")\n",
    "pivot_df = pivot_df.apply(lambda x: x.fillna(x.mean()), axis=0)  # Fill missing values\n",
    "pivot_df = pivot_df.head(10000)  # Restrict to 10000 time steps\n",
    "\n",
    "# Parameters\n",
    "reward = 0.5\n",
    "M = 1  # Maximum number of nodes that can be polled\n",
    "theta = 0.5  # Threshold for reward condition\n",
    "penalty = -0.5  # Penalty for polling when difference is <= theta\n",
    "initial_value = 20  # Initial estimate for last polled values\n",
    "\n",
    "# Set parameters\n",
    "beta_1 = 0.9  # dEWMA parameter for state value\n",
    "beta_2 = 0.9  # dEWMA parameter for rate of change\n",
    "\n",
    "# Extract column names, ensuring \"SN\" is excluded\n",
    "columns = [col for col in pivot_df.columns if col != \"SN\"]\n",
    "num_nodes = len(columns)  # Number of sensor nodes based on dataset columns\n",
    "num_time_steps = len(pivot_df)  # Total time steps based on dataset length\n",
    "\n",
    "# Define time windows (every 1000 time steps)\n",
    "window_size = 1000\n",
    "num_windows = num_time_steps // window_size\n",
    "window_names = [f\"t={i*window_size+1}-{(i+1)*window_size}\" for i in range(num_windows)]\n",
    "\n",
    "# Track counts per window\n",
    "window_counts = [{\n",
    "    'Category A': 0,\n",
    "    'Category B': 0\n",
    "} for _ in range(num_windows)]\n",
    "\n",
    "# Function to calculate Age of Incorrect Information (AoII) at the sink\n",
    "def calculate_aoii_sink(current_time, last_received_time, last_rate_of_change):\n",
    "    # Handle potential NaN or Inf values\n",
    "    time_diff = current_time - last_received_time\n",
    "    if np.isnan(last_rate_of_change) or np.isinf(last_rate_of_change):\n",
    "        return 0.0  # Default to zero if rate of change is invalid\n",
    "    return abs(time_diff * last_rate_of_change)\n",
    "\n",
    "# Helper function to update state using dEWMA\n",
    "def update_node_state_dewma(measured_value, last_state_value, last_rate_of_change, delta_t, beta_1, beta_2):\n",
    "    # Handle the case where delta_t is 0 to avoid division by zero\n",
    "    if delta_t == 0:\n",
    "        return measured_value, last_rate_of_change  # Return measured value and keep the last rate of change\n",
    "    \n",
    "    x1 = beta_1 * measured_value + (1 - beta_1) * (last_state_value + last_rate_of_change * delta_t)\n",
    "    x2 = beta_2 * (x1 - last_state_value) / delta_t + (1 - beta_2) * last_rate_of_change\n",
    "    return x1, x2\n",
    "\n",
    "# Helper function to calculate reward\n",
    "def calculate_reward(measured_value, last_state_value, theta, penalty):\n",
    "    if abs(measured_value - last_state_value) > theta:\n",
    "        return reward  # Reward\n",
    "    return penalty  # Penalty\n",
    "\n",
    "# Function to extract numeric node ID\n",
    "def extract_node_id(col_name):\n",
    "    digits = ''.join(filter(str.isdigit, col_name))\n",
    "    return int(digits) if digits else None  # Return None if no digits are found\n",
    "\n",
    "# Function to get node category\n",
    "def get_node_category(node_id):\n",
    "    if node_id is None:\n",
    "        return None\n",
    "    if 1 <= node_id <= 5:\n",
    "        return 'Category A'\n",
    "    elif 6 <= node_id <= 10:\n",
    "        return 'Category B'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Dynamic Penalty Update algorithm\n",
    "def dynamic_penalty_update(whittle_indices, M, current_lambda):\n",
    "    # Convert dictionary values to a list of whittle indices, handling any NaN values\n",
    "    c_values = []\n",
    "    for v in whittle_indices.values():\n",
    "        if np.isnan(v):\n",
    "            c_values.append(-float('inf'))  # Treat NaN as negative infinity (won't be polled)\n",
    "        else:\n",
    "            c_values.append(v)\n",
    "    \n",
    "    # Identify the set ℰ of nodes where c_i > λ(t)\n",
    "    eligible_nodes = [i for i, c_i in enumerate(c_values) if c_i > current_lambda]\n",
    "    \n",
    "    # If |ℰ| ≤ M, no penalty update needed\n",
    "    if len(eligible_nodes) <= M:\n",
    "        return current_lambda\n",
    "    \n",
    "    # Sort the c_i values in descending order\n",
    "    sorted_c_values = sorted(c_values, reverse=True)\n",
    "    \n",
    "    # Identify the M-th value\n",
    "    M_th_value = sorted_c_values[M-1]\n",
    "    \n",
    "    # Update penalty to the M-th value\n",
    "    new_lambda = M_th_value\n",
    "    \n",
    "    return new_lambda\n",
    "\n",
    "# Main function to simulate Whittle AoII with detailed window tracking\n",
    "def run_simulation_with_window_tracking(pivot_df, columns, M, theta, penalty):\n",
    "    cumulative_reward = 0  # Track total cumulative reward\n",
    "    last_update_times = {col: 0 for col in columns}  # Last update time for each node\n",
    "    state_node = {col: np.array([20.0, 0.1]) for col in columns}  # Node states\n",
    "    \n",
    "    # Initialize dynamic penalty (λ) to 0\n",
    "    aoii_penalty = 0.0\n",
    "    \n",
    "    # Track penalty evolution by window\n",
    "    window_penalties = [[] for _ in range(num_windows)]\n",
    "    \n",
    "    # Track nodes polled in each window\n",
    "    window_nodes_polled = [[] for _ in range(num_windows)]\n",
    "    \n",
    "    # Set minimum timestamp difference to avoid division by zero\n",
    "    min_delta_t = 1  # Minimum time difference of 1 to avoid division by zero\n",
    "\n",
    "    for t in range(len(pivot_df)):\n",
    "        # Determine current window\n",
    "        current_window = t // window_size\n",
    "        \n",
    "        # Step 1: Compute Whittle indices for each node based on AoII\n",
    "        whittle_indices = {}\n",
    "        for col in columns:\n",
    "            last_state_value, last_rate_of_change = state_node[col]\n",
    "            measured_value = pivot_df.loc[t, col]\n",
    "\n",
    "            # Correct AoII calculation at the sink using rate of change\n",
    "            current_aoii = calculate_aoii_sink(t, last_update_times[col], last_rate_of_change)\n",
    "            future_aoii_passive = calculate_aoii_sink(t+1, last_update_times[col], last_rate_of_change)\n",
    "            future_aoii_active = 0  # AoII resets to 0 if polled\n",
    "\n",
    "            # Whittle index calculations - with safety checks for NaN/Inf values\n",
    "            q_passive = current_aoii + future_aoii_passive\n",
    "            q_active = current_aoii + future_aoii_active + aoii_penalty\n",
    "            \n",
    "            # Calculate the Whittle index with safeguards against invalid values\n",
    "            whittle_index = q_passive - q_active\n",
    "            \n",
    "            # Handle NaN or Inf values\n",
    "            if np.isnan(whittle_index) or np.isinf(whittle_index):\n",
    "                whittle_indices[col] = -float('inf')  # Set to negative infinity if invalid\n",
    "            else:\n",
    "                whittle_indices[col] = whittle_index\n",
    "\n",
    "        # Step 2: Update the dynamic penalty (λ) using the algorithm\n",
    "        aoii_penalty = dynamic_penalty_update(whittle_indices, M, aoii_penalty)\n",
    "        \n",
    "        # Track penalty by window\n",
    "        window_penalties[current_window].append(aoii_penalty)\n",
    "        \n",
    "        # Step 3: Select nodes to poll based on the updated penalty\n",
    "        nodes_to_poll = [col for col in whittle_indices if whittle_indices[col] >= aoii_penalty]\n",
    "        window_nodes_polled[current_window].append(len(nodes_to_poll))\n",
    "\n",
    "        # Step 4: Poll selected nodes and calculate rewards\n",
    "        for col in nodes_to_poll:\n",
    "            measured_value = pivot_df.loc[t, col]\n",
    "            last_state_value, last_rate_of_change = state_node[col]\n",
    "\n",
    "            # Calculate reward after polling\n",
    "            reward = calculate_reward(measured_value, last_state_value, theta, penalty)\n",
    "            cumulative_reward += reward  # Update cumulative reward\n",
    "\n",
    "            delta_t_dynamic = max(min_delta_t, t - last_update_times[col])  # Time since last update (ensure at least 1)\n",
    "\n",
    "            # Update node state and last update time\n",
    "            state_node[col] = update_node_state_dewma(\n",
    "                measured_value, last_state_value, last_rate_of_change, delta_t_dynamic, beta_1=beta_1, beta_2=beta_2\n",
    "            )\n",
    "            last_update_times[col] = t\n",
    "\n",
    "            # Extract node ID dynamically and categorize\n",
    "            node_id = extract_node_id(col)\n",
    "            if node_id is not None:\n",
    "                category = get_node_category(node_id)\n",
    "                if category:\n",
    "                    # Increment appropriate window counter\n",
    "                    window_counts[current_window][category] += 1\n",
    "\n",
    "    # Calculate window statistics\n",
    "    window_stats = []\n",
    "    for i in range(num_windows):\n",
    "        total_polled = sum(window_counts[i].values())\n",
    "        category_a_pct = (window_counts[i]['Category A'] / total_polled * 100) if total_polled > 0 else 0\n",
    "        category_b_pct = (window_counts[i]['Category B'] / total_polled * 100) if total_polled > 0 else 0\n",
    "        \n",
    "        avg_penalty = np.mean(window_penalties[i]) if window_penalties[i] else 0\n",
    "        avg_nodes_polled = np.mean(window_nodes_polled[i]) if window_nodes_polled[i] else 0\n",
    "        \n",
    "        window_stats.append({\n",
    "            'window': window_names[i],\n",
    "            'Category A': window_counts[i]['Category A'],\n",
    "            'Category B': window_counts[i]['Category B'],\n",
    "            'Category A %': category_a_pct,\n",
    "            'Category B %': category_b_pct,\n",
    "            'Total Polled': total_polled,\n",
    "            'Avg Penalty': avg_penalty,\n",
    "            'Avg Nodes Polled': avg_nodes_polled\n",
    "        })\n",
    "\n",
    "    return window_stats, window_counts\n",
    "\n",
    "# Run the simulation with detailed window tracking\n",
    "window_stats, window_counts = run_simulation_with_window_tracking(\n",
    "    pivot_df, columns, M, theta, penalty\n",
    ")\n",
    "\n",
    "# Print detailed window statistics\n",
    "print(\"Polling Distribution by Time Window:\")\n",
    "print(\"----------------------------------\")\n",
    "for i, stats in enumerate(window_stats):\n",
    "    print(f\"\\nWindow {i+1}: {stats['window']}\")\n",
    "    print(f\"Category A: {stats['Category A']} times ({stats['Category A %']:.1f}%)\")\n",
    "    print(f\"Category B: {stats['Category B']} times ({stats['Category B %']:.1f}%)\")\n",
    "    print(f\"Total Polled: {stats['Total Polled']}\")\n",
    "    print(f\"Average Penalty: {stats['Avg Penalty']:.4f}\")\n",
    "    print(f\"Average Nodes Polled: {stats['Avg Nodes Polled']:.2f}\")\n",
    "\n",
    "# Print summary statistics in CSV-friendly format\n",
    "print(\"\\nWindow-by-Window Summary (CSV format):\")\n",
    "print(\"Window,Category A Count,Category B Count,Category A %,Category B %,Total Polled,Avg Penalty,Avg Nodes Polled\")\n",
    "for stats in window_stats:\n",
    "    print(f\"{stats['window']},{stats['Category A']},{stats['Category B']},{stats['Category A %']:.1f},{stats['Category B %']:.1f},{stats['Total Polled']},{stats['Avg Penalty']:.4f},{stats['Avg Nodes Polled']:.2f}\")\n",
    "\n",
    "# Print pre/post transition comparison\n",
    "pre_transition = window_stats[:5]  # Windows 1-5 (before t=5000)\n",
    "post_transition = window_stats[5:]  # Windows 6-10 (after t=5000)\n",
    "\n",
    "pre_cat_a = sum(stats['Category A'] for stats in pre_transition)\n",
    "pre_cat_b = sum(stats['Category B'] for stats in pre_transition)\n",
    "pre_total = pre_cat_a + pre_cat_b\n",
    "\n",
    "post_cat_a = sum(stats['Category A'] for stats in post_transition)\n",
    "post_cat_b = sum(stats['Category B'] for stats in post_transition)\n",
    "post_total = post_cat_a + post_cat_b\n",
    "\n",
    "print(\"\\nPre-Transition Summary (t=1-5000):\")\n",
    "print(f\"Category A: {pre_cat_a} times ({pre_cat_a/pre_total*100:.1f}%)\")\n",
    "print(f\"Category B: {pre_cat_b} times ({pre_cat_b/pre_total*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nPost-Transition Summary (t=5001-10000):\")\n",
    "print(f\"Category A: {post_cat_a} times ({post_cat_a/post_total*100:.1f}%)\")\n",
    "print(f\"Category B: {post_cat_b} times ({post_cat_b/post_total*100:.1f}%)\")\n",
    "\n",
    "# Print trend observation\n",
    "if pre_cat_a > pre_cat_b and post_cat_b > post_cat_a:\n",
    "    print(\"\\nObservation: The algorithm successfully adapted to the trend reversal at t=5000\")\n",
    "    print(f\"Pre-transition: Category A polled {pre_cat_a/pre_cat_b:.1f}x more than Category B\")\n",
    "    print(f\"Post-transition: Category B polled {post_cat_b/post_cat_a:.1f}x more than Category A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fairness Configuration: Check every 100 timesteps, Window size: 500\n",
      "\n",
      "Polling Distribution by Time Window:\n",
      "----------------------------------\n",
      "\n",
      "Window 1: t=1-1000\n",
      "Category A: 129 times (89.6%)\n",
      "Category B: 15 times (10.4%)\n",
      "Total Polled: 144\n",
      "Fairness-triggered polls: 2 (1.4% of total)\n",
      "   - Category A: 1 (50.0% of fairness)\n",
      "   - Category B: 1 (50.0% of fairness)\n",
      "Average Penalty: 0.5617\n",
      "Average Nodes Polled: 0.14\n",
      "\n",
      "Window 2: t=1001-2000\n",
      "Category A: 52 times (88.1%)\n",
      "Category B: 7 times (11.9%)\n",
      "Total Polled: 59\n",
      "Fairness-triggered polls: 10 (16.9% of total)\n",
      "   - Category A: 3 (30.0% of fairness)\n",
      "   - Category B: 7 (70.0% of fairness)\n",
      "Average Penalty: 0.6355\n",
      "Average Nodes Polled: 0.06\n",
      "\n",
      "Window 3: t=2001-3000\n",
      "Category A: 45 times (84.9%)\n",
      "Category B: 8 times (15.1%)\n",
      "Total Polled: 53\n",
      "Fairness-triggered polls: 10 (18.9% of total)\n",
      "   - Category A: 2 (20.0% of fairness)\n",
      "   - Category B: 8 (80.0% of fairness)\n",
      "Average Penalty: 0.6467\n",
      "Average Nodes Polled: 0.05\n",
      "\n",
      "Window 4: t=3001-4000\n",
      "Category A: 68 times (90.7%)\n",
      "Category B: 7 times (9.3%)\n",
      "Total Polled: 75\n",
      "Fairness-triggered polls: 10 (13.3% of total)\n",
      "   - Category A: 3 (30.0% of fairness)\n",
      "   - Category B: 7 (70.0% of fairness)\n",
      "Average Penalty: 0.6690\n",
      "Average Nodes Polled: 0.07\n",
      "\n",
      "Window 5: t=4001-5000\n",
      "Category A: 57 times (87.7%)\n",
      "Category B: 8 times (12.3%)\n",
      "Total Polled: 65\n",
      "Fairness-triggered polls: 9 (13.8% of total)\n",
      "   - Category A: 1 (11.1% of fairness)\n",
      "   - Category B: 8 (88.9% of fairness)\n",
      "Average Penalty: 0.7379\n",
      "Average Nodes Polled: 0.07\n",
      "\n",
      "Window 6: t=5001-6000\n",
      "Category A: 16 times (43.2%)\n",
      "Category B: 21 times (56.8%)\n",
      "Total Polled: 37\n",
      "Fairness-triggered polls: 10 (27.0% of total)\n",
      "   - Category A: 4 (40.0% of fairness)\n",
      "   - Category B: 6 (60.0% of fairness)\n",
      "Average Penalty: 0.7595\n",
      "Average Nodes Polled: 0.04\n",
      "\n",
      "Window 7: t=6001-7000\n",
      "Category A: 6 times (12.2%)\n",
      "Category B: 43 times (87.8%)\n",
      "Total Polled: 49\n",
      "Fairness-triggered polls: 9 (18.4% of total)\n",
      "   - Category A: 5 (55.6% of fairness)\n",
      "   - Category B: 4 (44.4% of fairness)\n",
      "Average Penalty: 0.7595\n",
      "Average Nodes Polled: 0.05\n",
      "\n",
      "Window 8: t=7001-8000\n",
      "Category A: 6 times (15.8%)\n",
      "Category B: 32 times (84.2%)\n",
      "Total Polled: 38\n",
      "Fairness-triggered polls: 10 (26.3% of total)\n",
      "   - Category A: 6 (60.0% of fairness)\n",
      "   - Category B: 4 (40.0% of fairness)\n",
      "Average Penalty: 0.7595\n",
      "Average Nodes Polled: 0.04\n",
      "\n",
      "Window 9: t=8001-9000\n",
      "Category A: 8 times (12.3%)\n",
      "Category B: 57 times (87.7%)\n",
      "Total Polled: 65\n",
      "Fairness-triggered polls: 10 (15.4% of total)\n",
      "   - Category A: 8 (80.0% of fairness)\n",
      "   - Category B: 2 (20.0% of fairness)\n",
      "Average Penalty: 0.7895\n",
      "Average Nodes Polled: 0.07\n",
      "\n",
      "Window 10: t=9001-10000\n",
      "Category A: 8 times (16.0%)\n",
      "Category B: 42 times (84.0%)\n",
      "Total Polled: 50\n",
      "Fairness-triggered polls: 9 (18.0% of total)\n",
      "   - Category A: 8 (88.9% of fairness)\n",
      "   - Category B: 1 (11.1% of fairness)\n",
      "Average Penalty: 0.8001\n",
      "Average Nodes Polled: 0.05\n",
      "\n",
      "Window-by-Window Summary (CSV format):\n",
      "Window,Cat A,Cat B,Cat A %,Cat B %,Total,Fairness A,Fairness B,Fairness %,Avg Penalty,Avg Nodes Polled\n",
      "t=1-1000,129,15,89.6,10.4,144,1,1,1.4,0.5617,0.14\n",
      "t=1001-2000,52,7,88.1,11.9,59,3,7,16.9,0.6355,0.06\n",
      "t=2001-3000,45,8,84.9,15.1,53,2,8,18.9,0.6467,0.05\n",
      "t=3001-4000,68,7,90.7,9.3,75,3,7,13.3,0.6690,0.07\n",
      "t=4001-5000,57,8,87.7,12.3,65,1,8,13.8,0.7379,0.07\n",
      "t=5001-6000,16,21,43.2,56.8,37,4,6,27.0,0.7595,0.04\n",
      "t=6001-7000,6,43,12.2,87.8,49,5,4,18.4,0.7595,0.05\n",
      "t=7001-8000,6,32,15.8,84.2,38,6,4,26.3,0.7595,0.04\n",
      "t=8001-9000,8,57,12.3,87.7,65,8,2,15.4,0.7895,0.07\n",
      "t=9001-10000,8,42,16.0,84.0,50,8,1,18.0,0.8001,0.05\n",
      "\n",
      "Pre-Transition Summary (t=1-5000):\n",
      "Category A: 351 times (88.6%)\n",
      "Category B: 45 times (11.4%)\n",
      "Fairness-triggered polls: 41 (10.4% of total)\n",
      "\n",
      "Post-Transition Summary (t=5001-10000):\n",
      "Category A: 44 times (18.4%)\n",
      "Category B: 195 times (81.6%)\n",
      "Fairness-triggered polls: 48 (20.1% of total)\n",
      "\n",
      "Fairness Mechanism Impact:\n",
      "Total polls generated by fairness: 89\n",
      "Percentage of all polling due to fairness: 14.0%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset and preprocess\n",
    "pivot_df = pd.read_csv(\"synthetic_temp_polling_data_reversed.csv\")\n",
    "#pivot_df = pd.read_csv(\"synthetic_temp_polling_data.csv\")\n",
    "pivot_df = pivot_df.apply(lambda x: x.fillna(x.mean()), axis=0)  # Fill missing values\n",
    "pivot_df = pivot_df.head(10000)  # Restrict to 10000 time steps\n",
    "\n",
    "# Parameters\n",
    "reward = 0.5\n",
    "M = 1  # Maximum number of nodes that can be polled\n",
    "theta = 0.5  # Threshold for reward condition\n",
    "penalty = -0.5  # Penalty for polling when difference is <= theta\n",
    "initial_value = 20  # Initial estimate for last polled values\n",
    "\n",
    "# Set parameters\n",
    "beta_1 = 0.9 # dEWMA parameter for state value\n",
    "beta_2 = 0.9  # dEWMA parameter for rate of change\n",
    "\n",
    "# Fairness parameters\n",
    "L = 100  # Force fairness check every L time steps\n",
    "fairness_window = 500  # Consider nodes not polled in this window for fairness polling\n",
    "\n",
    "# Extract column names, ensuring \"SN\" is excluded\n",
    "columns = [col for col in pivot_df.columns if col != \"SN\"]\n",
    "num_nodes = len(columns)  # Number of sensor nodes based on dataset columns\n",
    "num_time_steps = len(pivot_df)  # Total time steps based on dataset length\n",
    "\n",
    "# Define time windows (every 1000 time steps)\n",
    "window_size = 1000\n",
    "num_windows = num_time_steps // window_size\n",
    "window_names = [f\"t={i*window_size+1}-{(i+1)*window_size}\" for i in range(num_windows)]\n",
    "\n",
    "# Track counts per window\n",
    "window_counts = [{\n",
    "    'Category A': 0,\n",
    "    'Category B': 0\n",
    "} for _ in range(num_windows)]\n",
    "\n",
    "# Track fairness-triggered polls\n",
    "fairness_poll_counts = [{\n",
    "    'Category A': 0,\n",
    "    'Category B': 0\n",
    "} for _ in range(num_windows)]\n",
    "\n",
    "# Function to calculate Age of Incorrect Information (AoII) at the sink\n",
    "def calculate_aoii_sink(current_time, last_received_time, last_rate_of_change):\n",
    "    # Handle potential NaN or Inf values\n",
    "    time_diff = current_time - last_received_time\n",
    "    if np.isnan(last_rate_of_change) or np.isinf(last_rate_of_change):\n",
    "        return 0.0  # Default to zero if rate of change is invalid\n",
    "    return abs(time_diff * last_rate_of_change)\n",
    "\n",
    "# Helper function to update state using dEWMA\n",
    "def update_node_state_dewma(measured_value, last_state_value, last_rate_of_change, delta_t, beta_1, beta_2):\n",
    "    # Handle the case where delta_t is 0 to avoid division by zero\n",
    "    if delta_t == 0:\n",
    "        return measured_value, last_rate_of_change  # Return measured value and keep the last rate of change\n",
    "    \n",
    "    x1 = beta_1 * measured_value + (1 - beta_1) * (last_state_value + last_rate_of_change * delta_t)\n",
    "    x2 = beta_2 * (x1 - last_state_value) / delta_t + (1 - beta_2) * last_rate_of_change\n",
    "    return x1, x2\n",
    "\n",
    "# Helper function to calculate reward\n",
    "def calculate_reward(measured_value, last_state_value, theta, penalty):\n",
    "    if abs(measured_value - last_state_value) > theta:\n",
    "        return reward  # Reward\n",
    "    return penalty  # Penalty\n",
    "\n",
    "# Function to extract numeric node ID\n",
    "def extract_node_id(col_name):\n",
    "    digits = ''.join(filter(str.isdigit, col_name))\n",
    "    return int(digits) if digits else None  # Return None if no digits are found\n",
    "\n",
    "# Function to get node category\n",
    "def get_node_category(node_id):\n",
    "    if node_id is None:\n",
    "        return None\n",
    "    if 1 <= node_id <= 5:\n",
    "        return 'Category A'\n",
    "    elif 6 <= node_id <= 10:\n",
    "        return 'Category B'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Dynamic Penalty Update algorithm\n",
    "def dynamic_penalty_update(whittle_indices, M, current_lambda):\n",
    "    # Convert dictionary values to a list of whittle indices, handling any NaN values\n",
    "    c_values = []\n",
    "    for v in whittle_indices.values():\n",
    "        if np.isnan(v):\n",
    "            c_values.append(-float('inf'))  # Treat NaN as negative infinity (won't be polled)\n",
    "        else:\n",
    "            c_values.append(v)\n",
    "    \n",
    "    # Identify the set ℰ of nodes where c_i > λ(t)\n",
    "    eligible_nodes = [i for i, c_i in enumerate(c_values) if c_i > current_lambda]\n",
    "    \n",
    "    # If |ℰ| ≤ M, no penalty update needed\n",
    "    if len(eligible_nodes) <= M:\n",
    "        return current_lambda\n",
    "    \n",
    "    # Sort the c_i values in descending order\n",
    "    sorted_c_values = sorted(c_values, reverse=True)\n",
    "    \n",
    "    # Identify the M-th value\n",
    "    M_th_value = sorted_c_values[M-1]\n",
    "    \n",
    "    # Update penalty to the M-th value\n",
    "    new_lambda = M_th_value\n",
    "    \n",
    "    return new_lambda\n",
    "\n",
    "# Helper function to poll a node and update state\n",
    "def poll_node(col, t, measured_value, state_node, last_update_times, is_fairness_poll=False):\n",
    "    last_state_value, last_rate_of_change = state_node[col]\n",
    "    \n",
    "    # Get min_delta_t to avoid division by zero\n",
    "    min_delta_t = 1\n",
    "    delta_t_dynamic = max(min_delta_t, t - last_update_times[col])\n",
    "    \n",
    "    # Update node state and last update time\n",
    "    state_node[col] = update_node_state_dewma(\n",
    "        measured_value, last_state_value, last_rate_of_change, delta_t_dynamic, beta_1=beta_1, beta_2=beta_2\n",
    "    )\n",
    "    last_update_times[col] = t\n",
    "    \n",
    "    # Track category of polled node\n",
    "    node_id = extract_node_id(col)\n",
    "    if node_id is not None:\n",
    "        category = get_node_category(node_id)\n",
    "        current_window = t // window_size\n",
    "        \n",
    "        if category:\n",
    "            # Update appropriate counter\n",
    "            if is_fairness_poll:\n",
    "                fairness_poll_counts[current_window][category] += 1\n",
    "            \n",
    "            # Update total counter regardless of poll type\n",
    "            window_counts[current_window][category] += 1\n",
    "    \n",
    "    return state_node, last_update_times\n",
    "\n",
    "# Main function to simulate Whittle AoII with fairness mechanism\n",
    "def run_simulation_with_fairness(pivot_df, columns, M, theta, penalty, L, fairness_window):\n",
    "    cumulative_reward = 0  # Track total cumulative reward\n",
    "    last_update_times = {col: 0 for col in columns}  # Last update time for each node\n",
    "    state_node = {col: np.array([20.0, 0.1]) for col in columns}  # Node states\n",
    "    \n",
    "    # Initialize dynamic penalty (λ) to 0\n",
    "    aoii_penalty = 0.0\n",
    "    \n",
    "    # Track penalty evolution by window\n",
    "    window_penalties = [[] for _ in range(num_windows)]\n",
    "    \n",
    "    # Track nodes polled in each window\n",
    "    window_nodes_polled = [[] for _ in range(num_windows)]\n",
    "    window_fairness_polled = [[] for _ in range(num_windows)]\n",
    "    \n",
    "    # Set minimum timestamp difference to avoid division by zero\n",
    "    min_delta_t = 1  # Minimum time difference of 1 to avoid division by zero\n",
    "\n",
    "    for t in range(len(pivot_df)):\n",
    "        # Determine current window\n",
    "        current_window = t // window_size\n",
    "        \n",
    "        # Step 1: Fairness check (every L time steps)\n",
    "        fairness_polls = []\n",
    "        if t % L == 0:  # Every L time steps\n",
    "            # Find nodes that haven't been polled recently\n",
    "            neglected_nodes = []\n",
    "            for col in columns:\n",
    "                if t - last_update_times[col] > fairness_window:\n",
    "                    neglected_nodes.append(col)\n",
    "            \n",
    "            # Sort by last update time (oldest first)\n",
    "            neglected_nodes.sort(key=lambda col: last_update_times[col])\n",
    "            \n",
    "            # Take up to M nodes for fairness polling\n",
    "            fairness_polls = neglected_nodes[:M]\n",
    "            window_fairness_polled[current_window].append(len(fairness_polls))\n",
    "            \n",
    "            # Poll the fairness nodes\n",
    "            for col in fairness_polls:\n",
    "                measured_value = pivot_df.loc[t, col]\n",
    "                state_node, last_update_times = poll_node(\n",
    "                    col, t, measured_value, state_node, last_update_times, \n",
    "                    is_fairness_poll=True\n",
    "                )\n",
    "        else:\n",
    "            window_fairness_polled[current_window].append(0)\n",
    "        \n",
    "        # Step 2: Regular Whittle index calculation and polling\n",
    "        # Only consider nodes not already polled by fairness mechanism\n",
    "        remaining_slots = M - len(fairness_polls)\n",
    "        \n",
    "        if remaining_slots > 0:\n",
    "            whittle_indices = {}\n",
    "            for col in columns:\n",
    "                # Skip nodes already polled by fairness\n",
    "                if col in fairness_polls:\n",
    "                    continue\n",
    "                    \n",
    "                last_state_value, last_rate_of_change = state_node[col]\n",
    "                measured_value = pivot_df.loc[t, col]\n",
    "\n",
    "                # Calculate AoII\n",
    "                current_aoii = calculate_aoii_sink(t, last_update_times[col], last_rate_of_change)\n",
    "                future_aoii_passive = calculate_aoii_sink(t+1, last_update_times[col], last_rate_of_change)\n",
    "                future_aoii_active = 0  # AoII resets to 0 if polled\n",
    "\n",
    "                # Whittle index calculations\n",
    "                q_passive = current_aoii + future_aoii_passive\n",
    "                q_active = current_aoii + future_aoii_active + aoii_penalty\n",
    "                \n",
    "                whittle_index = q_passive - q_active\n",
    "                \n",
    "                # Handle NaN or Inf values\n",
    "                if np.isnan(whittle_index) or np.isinf(whittle_index):\n",
    "                    whittle_indices[col] = -float('inf')  # Set to negative infinity if invalid\n",
    "                else:\n",
    "                    whittle_indices[col] = whittle_index\n",
    "\n",
    "            # Update the dynamic penalty (λ)\n",
    "            aoii_penalty = dynamic_penalty_update(whittle_indices, remaining_slots, aoii_penalty)\n",
    "            window_penalties[current_window].append(aoii_penalty)\n",
    "            \n",
    "            # Select nodes to poll based on updated penalty\n",
    "            nodes_to_poll = [col for col in whittle_indices if whittle_indices[col] >= aoii_penalty]\n",
    "            if len(nodes_to_poll) > remaining_slots:\n",
    "                nodes_to_poll = sorted(nodes_to_poll, key=whittle_indices.get, reverse=True)[:remaining_slots]\n",
    "            \n",
    "            window_nodes_polled[current_window].append(len(fairness_polls) + len(nodes_to_poll))\n",
    "            \n",
    "            # Poll the selected nodes\n",
    "            for col in nodes_to_poll:\n",
    "                measured_value = pivot_df.loc[t, col]\n",
    "                state_node, last_update_times = poll_node(\n",
    "                    col, t, measured_value, state_node, last_update_times\n",
    "                )\n",
    "        else:\n",
    "            window_penalties[current_window].append(aoii_penalty)\n",
    "            window_nodes_polled[current_window].append(len(fairness_polls))\n",
    "\n",
    "    # Calculate window statistics\n",
    "    window_stats = []\n",
    "    for i in range(num_windows):\n",
    "        total_polled = sum(window_counts[i].values())\n",
    "        fairness_polled = sum(fairness_poll_counts[i].values())\n",
    "        \n",
    "        category_a_pct = (window_counts[i]['Category A'] / total_polled * 100) if total_polled > 0 else 0\n",
    "        category_b_pct = (window_counts[i]['Category B'] / total_polled * 100) if total_polled > 0 else 0\n",
    "        \n",
    "        fairness_a_pct = (fairness_poll_counts[i]['Category A'] / fairness_polled * 100) if fairness_polled > 0 else 0\n",
    "        fairness_b_pct = (fairness_poll_counts[i]['Category B'] / fairness_polled * 100) if fairness_polled > 0 else 0\n",
    "        \n",
    "        avg_penalty = np.mean(window_penalties[i]) if window_penalties[i] else 0\n",
    "        avg_nodes_polled = np.mean(window_nodes_polled[i]) if window_nodes_polled[i] else 0\n",
    "        avg_fairness_polled = np.mean(window_fairness_polled[i]) if window_fairness_polled[i] else 0\n",
    "        \n",
    "        window_stats.append({\n",
    "            'window': window_names[i],\n",
    "            'Category A': window_counts[i]['Category A'],\n",
    "            'Category B': window_counts[i]['Category B'],\n",
    "            'Category A %': category_a_pct,\n",
    "            'Category B %': category_b_pct,\n",
    "            'Total Polled': total_polled,\n",
    "            'Fairness A': fairness_poll_counts[i]['Category A'],\n",
    "            'Fairness B': fairness_poll_counts[i]['Category B'],\n",
    "            'Fairness A %': fairness_a_pct,\n",
    "            'Fairness B %': fairness_b_pct,\n",
    "            'Total Fairness': fairness_polled,\n",
    "            'Fairness %': (fairness_polled / total_polled * 100) if total_polled > 0 else 0,\n",
    "            'Avg Penalty': avg_penalty,\n",
    "            'Avg Nodes Polled': avg_nodes_polled,\n",
    "            'Avg Fairness Polled': avg_fairness_polled\n",
    "        })\n",
    "\n",
    "    return window_stats, window_counts, fairness_poll_counts\n",
    "\n",
    "# Run the simulation with fairness mechanism\n",
    "window_stats, window_counts, fairness_poll_counts = run_simulation_with_fairness(\n",
    "    pivot_df, columns, M, theta, penalty, L, fairness_window\n",
    ")\n",
    "\n",
    "# Print detailed window statistics\n",
    "print(f\"Fairness Configuration: Check every {L} timesteps, Window size: {fairness_window}\")\n",
    "print(\"\\nPolling Distribution by Time Window:\")\n",
    "print(\"----------------------------------\")\n",
    "for i, stats in enumerate(window_stats):\n",
    "    print(f\"\\nWindow {i+1}: {stats['window']}\")\n",
    "    print(f\"Category A: {stats['Category A']} times ({stats['Category A %']:.1f}%)\")\n",
    "    print(f\"Category B: {stats['Category B']} times ({stats['Category B %']:.1f}%)\")\n",
    "    print(f\"Total Polled: {stats['Total Polled']}\")\n",
    "    print(f\"Fairness-triggered polls: {stats['Total Fairness']} ({stats['Fairness %']:.1f}% of total)\")\n",
    "    print(f\"   - Category A: {stats['Fairness A']} ({stats['Fairness A %']:.1f}% of fairness)\")\n",
    "    print(f\"   - Category B: {stats['Fairness B']} ({stats['Fairness B %']:.1f}% of fairness)\")\n",
    "    print(f\"Average Penalty: {stats['Avg Penalty']:.4f}\")\n",
    "    print(f\"Average Nodes Polled: {stats['Avg Nodes Polled']:.2f}\")\n",
    "\n",
    "# Print summary statistics in CSV-friendly format\n",
    "print(\"\\nWindow-by-Window Summary (CSV format):\")\n",
    "print(\"Window,Cat A,Cat B,Cat A %,Cat B %,Total,Fairness A,Fairness B,Fairness %,Avg Penalty,Avg Nodes Polled\")\n",
    "for stats in window_stats:\n",
    "    print(f\"{stats['window']},{stats['Category A']},{stats['Category B']},{stats['Category A %']:.1f},{stats['Category B %']:.1f},{stats['Total Polled']},{stats['Fairness A']},{stats['Fairness B']},{stats['Fairness %']:.1f},{stats['Avg Penalty']:.4f},{stats['Avg Nodes Polled']:.2f}\")\n",
    "\n",
    "# Print pre/post transition comparison\n",
    "pre_transition = window_stats[:5]  # Windows 1-5 (before t=5000)\n",
    "post_transition = window_stats[5:]  # Windows 6-10 (after t=5000)\n",
    "\n",
    "pre_cat_a = sum(stats['Category A'] for stats in pre_transition)\n",
    "pre_cat_b = sum(stats['Category B'] for stats in pre_transition)\n",
    "pre_total = pre_cat_a + pre_cat_b\n",
    "\n",
    "post_cat_a = sum(stats['Category A'] for stats in post_transition)\n",
    "post_cat_b = sum(stats['Category B'] for stats in post_transition)\n",
    "post_total = post_cat_a + post_cat_b\n",
    "\n",
    "pre_fairness = sum(stats['Total Fairness'] for stats in pre_transition)\n",
    "post_fairness = sum(stats['Total Fairness'] for stats in post_transition)\n",
    "\n",
    "print(\"\\nPre-Transition Summary (t=1-5000):\")\n",
    "print(f\"Category A: {pre_cat_a} times ({pre_cat_a/pre_total*100:.1f}%)\")\n",
    "print(f\"Category B: {pre_cat_b} times ({pre_cat_b/pre_total*100:.1f}%)\")\n",
    "print(f\"Fairness-triggered polls: {pre_fairness} ({pre_fairness/pre_total*100:.1f}% of total)\")\n",
    "\n",
    "print(\"\\nPost-Transition Summary (t=5001-10000):\")\n",
    "print(f\"Category A: {post_cat_a} times ({post_cat_a/post_total*100:.1f}%)\")\n",
    "print(f\"Category B: {post_cat_b} times ({post_cat_b/post_total*100:.1f}%)\")\n",
    "print(f\"Fairness-triggered polls: {post_fairness} ({post_fairness/post_total*100:.1f}% of total)\")\n",
    "\n",
    "# Print observations about fairness mechanism\n",
    "print(\"\\nFairness Mechanism Impact:\")\n",
    "print(f\"Total polls generated by fairness: {pre_fairness + post_fairness}\")\n",
    "print(f\"Percentage of all polling due to fairness: {(pre_fairness + post_fairness)/(pre_total + post_total)*100:.1f}%\")\n",
    "\n",
    "if post_cat_b > post_cat_a and (post_fairness/post_total*100) > 50:\n",
    "    print(\"Observation: The fairness mechanism was critical in detecting the trend reversal after t=5000\")\n",
    "    print(f\"Post-transition fairness polling accounted for {post_fairness/post_total*100:.1f}% of all polls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transmission Count by Category:\n",
      "Category A: 1021 times\n",
      "Category B: 20 times\n",
      "\n",
      "Total MSE: 0.004706\n",
      "\n",
      "MSE by Category:\n",
      "Category A: 0.004505\n",
      "Category B: 0.000842\n",
      "\n",
      "--- Detailed MSE Results ---\n",
      "Total nodes analyzed: 10\n",
      "Average MSE across all nodes: 0.002674\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#pivot_df = pd.read_csv(\"synthetic_scenario_30_nodes.csv\")\n",
    "pivot_df = pd.read_csv(\"synthetic_temp_polling_data.csv\")\n",
    "pivot_df = pivot_df.head(20000)  # Restrict to first 20000 time steps\n",
    "\n",
    "M = 2  # Maximum number of nodes that can be polled\n",
    "aoii_penalty = 0.5\n",
    "initial_value = 20  # Initial estimate for last polled values\n",
    "\n",
    "# Set parameters\n",
    "beta_1 = 0.95  # dEWMA parameter for state value\n",
    "beta_2 = 0.9 # dEWMA parameter for rate of change\n",
    "\n",
    "# Extract column names, ensuring \"SN\" is excluded\n",
    "columns = [col for col in pivot_df.columns if col != \"SN\"]\n",
    "num_nodes = len(columns)  # Number of sensor nodes based on dataset columns\n",
    "num_time_steps = len(pivot_df)  # Total time steps based on dataset length\n",
    "\n",
    "# Track the number of times each category is pulled\n",
    "category_counts = {'Category A': 0, 'Category B': 0}\n",
    "\n",
    "# Function to calculate Age of Incorrect Information (AoII) at the sink\n",
    "def calculate_aoii_sink(current_time, last_received_time, last_rate_of_change):\n",
    "    return abs((current_time - last_received_time) * last_rate_of_change)\n",
    "\n",
    "# Helper function to update state using dEWMA\n",
    "def update_node_state_dewma(measured_value, last_state_value, last_rate_of_change, delta_t, beta_1, beta_2):\n",
    "    # Handle the case where delta_t is 0 to avoid division by zero\n",
    "    if delta_t == 0:\n",
    "        return measured_value, last_rate_of_change\n",
    "        \n",
    "    x1 = beta_1 * measured_value + (1 - beta_1) * (last_state_value + last_rate_of_change * delta_t)\n",
    "    x2 = beta_2 * (x1 - last_state_value) / delta_t + (1 - beta_2) * last_rate_of_change\n",
    "    return x1, x2\n",
    "\n",
    "# Function to extract numeric node ID from column names dynamically, ensuring valid extraction\n",
    "def extract_node_id(col_name):\n",
    "    \"\"\"Extract numeric node ID from column name, handling cases where no digits are found.\"\"\"\n",
    "    digits = ''.join(filter(str.isdigit, col_name))\n",
    "    return int(digits) if digits else None  # Return None if no digits are found\n",
    "\n",
    "# Function to get node category\n",
    "def get_node_category(node_id):\n",
    "    if node_id is None:\n",
    "        return None\n",
    "    if 1 <= node_id <= 5:\n",
    "        return 'Category A'\n",
    "    elif 6 <= node_id <= 10:\n",
    "        return 'Category B'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Main function to simulate Whittle AoII with rewards and track transmission counts\n",
    "def run_simulation_whittle_aoii(pivot_df, columns, M, aoii_penalty):\n",
    "    \n",
    "    last_update_times = {col: 0 for col in columns}  # Last update time for each node\n",
    "    state_node = {col: np.array([20.0, 0.1]) for col in columns}  # Node states\n",
    "    \n",
    "    # Initialize storage for MSE calculation\n",
    "    true_values = {col: [] for col in columns}\n",
    "    predicted_values = {col: [] for col in columns}\n",
    "    \n",
    "    # Set minimum timestamp difference to avoid division by zero\n",
    "    min_delta_t = 1\n",
    "\n",
    "    for t in range(len(pivot_df)):\n",
    "        # Step 1: Compute Whittle indices for each node based on AoII\n",
    "        whittle_indices = {}\n",
    "        for col in columns:\n",
    "            last_state_value, last_rate_of_change = state_node[col]\n",
    "            measured_value = pivot_df.loc[t, col]\n",
    "            \n",
    "            # Store true value for all nodes at each time step\n",
    "            \n",
    "            \n",
    "            # Correct AoII calculation at the sink using rate of change\n",
    "            current_aoii = calculate_aoii_sink(t, last_update_times[col], last_rate_of_change)\n",
    "            future_aoii_passive = calculate_aoii_sink(t+1, last_update_times[col], last_rate_of_change)\n",
    "            future_aoii_active = 0  # AoII resets to 0 if polled\n",
    "\n",
    "            # Whittle index calculations\n",
    "            q_passive = current_aoii + future_aoii_passive\n",
    "            q_active = current_aoii + future_aoii_active + aoii_penalty\n",
    "            whittle_indices[col] = q_passive - q_active\n",
    "\n",
    "        # Step 2: Select top M nodes to poll based on Whittle indices\n",
    "        nodes_to_poll = [col for col in whittle_indices if whittle_indices[col] >= 0]\n",
    "        if len(nodes_to_poll) > M:\n",
    "            nodes_to_poll = sorted(nodes_to_poll, key=whittle_indices.get, reverse=True)[:M]\n",
    "\n",
    "        # Step 3: Poll selected nodes and calculate rewards\n",
    "        for col in nodes_to_poll:\n",
    "            measured_value = pivot_df.loc[t, col]\n",
    "            last_state_value, last_rate_of_change = state_node[col]\n",
    "\n",
    "            delta_t_dynamic = max(min_delta_t, t - last_update_times[col])  # Time since last update\n",
    "\n",
    "            # Update node state and last update time\n",
    "            state_node[col] = update_node_state_dewma(\n",
    "                measured_value, last_state_value, last_rate_of_change, delta_t_dynamic, beta_1=beta_1, beta_2=beta_2\n",
    "            )\n",
    "            last_state_value, last_rate_of_change = state_node[col]\n",
    "            predicted_values[col].append(last_state_value)\n",
    "            true_values[col].append(measured_value)\n",
    "            last_update_times[col] = t\n",
    "\n",
    "            # Extract node ID dynamically and categorize\n",
    "            node_id = extract_node_id(col)\n",
    "            if node_id is not None:\n",
    "                if 1 <= node_id <= 5:\n",
    "                    category_counts['Category A'] += 1\n",
    "                elif 6 <= node_id <= 10:\n",
    "                    category_counts['Category B'] += 1\n",
    "\n",
    "    return category_counts, true_values, predicted_values\n",
    "\n",
    "# Run the simulation\n",
    "category_pulled_counts, true_values, predicted_values = run_simulation_whittle_aoii(\n",
    "    pivot_df, columns, M, aoii_penalty\n",
    ")\n",
    "\n",
    "# Print the number of times each category was pulled\n",
    "print(\"Transmission Count by Category:\")\n",
    "for category, count in category_pulled_counts.items():\n",
    "    print(f\"{category}: {count} times\")\n",
    "\n",
    "# Calculate MSE for each node\n",
    "node_mse = {}\n",
    "for col in columns:\n",
    "    # Handle potential NaN values\n",
    "    pred_array = np.array(predicted_values[col])\n",
    "    true_array = np.array(true_values[col])\n",
    "    \n",
    "    # Create mask for non-NaN values\n",
    "    valid_mask = ~np.isnan(pred_array) & ~np.isnan(true_array) & ~np.isinf(pred_array) & ~np.isinf(true_array)\n",
    "    \n",
    "    # Only calculate MSE with valid values\n",
    "    if np.any(valid_mask):\n",
    "        node_mse[col] = mean_squared_error(\n",
    "            true_array[valid_mask], \n",
    "            pred_array[valid_mask]\n",
    "        )\n",
    "    else:\n",
    "        node_mse[col] = np.nan\n",
    "\n",
    "# Calculate total MSE across all nodes\n",
    "all_true_values = []\n",
    "all_predicted_values = []\n",
    "\n",
    "for col in columns:\n",
    "    # Get arrays for this column\n",
    "    pred_array = np.array(predicted_values[col])\n",
    "    true_array = np.array(true_values[col])\n",
    "    \n",
    "    # Filter out NaN and Inf values\n",
    "    valid_mask = ~np.isnan(pred_array) & ~np.isnan(true_array) & ~np.isinf(pred_array) & ~np.isinf(true_array)\n",
    "    \n",
    "    # Add valid values to the combined arrays\n",
    "    all_true_values.extend(true_array[valid_mask])\n",
    "    all_predicted_values.extend(pred_array[valid_mask])\n",
    "\n",
    "# Calculate overall MSE if we have valid data\n",
    "if all_true_values and all_predicted_values:\n",
    "    total_mse = mean_squared_error(all_true_values, all_predicted_values)\n",
    "    print(f\"\\nTotal MSE: {total_mse:.6f}\")\n",
    "else:\n",
    "    print(\"\\nUnable to calculate Total MSE: No valid data points\")\n",
    "\n",
    "# Calculate MSE per category\n",
    "category_mse = {'Category A': [], 'Category B': []}\n",
    "for col in columns:\n",
    "    node_id = extract_node_id(col)\n",
    "    category = get_node_category(node_id)\n",
    "    if category and not np.isnan(node_mse[col]):\n",
    "        category_mse[category].append(node_mse[col])\n",
    "\n",
    "# Print MSE per category\n",
    "print(\"\\nMSE by Category:\")\n",
    "for category, mse_values in category_mse.items():\n",
    "    if mse_values:\n",
    "        avg_mse = np.mean(mse_values)\n",
    "        print(f\"{category}: {avg_mse:.6f}\")\n",
    "    else:\n",
    "        print(f\"{category}: No valid data for MSE calculation\")\n",
    "\n",
    "# Print detailed stats\n",
    "print(\"\\n--- Detailed MSE Results ---\")\n",
    "print(\"Total nodes analyzed:\", len(columns))\n",
    "valid_mse_values = [val for val in node_mse.values() if not np.isnan(val)]\n",
    "if valid_mse_values:\n",
    "    avg_node_mse = np.mean(valid_mse_values)\n",
    "    print(f\"Average MSE across all nodes: {avg_node_mse:.6f}\")\n",
    "else:\n",
    "    print(\"Average MSE across all nodes: Unable to calculate (all values are NaN)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transmission Count by Category:\n",
      "Category A: 5321 times\n",
      "Category B: 95 times\n",
      "\n",
      "Total MSE: 0.091690\n",
      "\n",
      "MSE by Category:\n",
      "Category A: 0.182191\n",
      "Category B: 0.001188\n",
      "\n",
      "--- Detailed MSE Results ---\n",
      "Total nodes analyzed: 10\n",
      "Average MSE across all nodes: 0.091690\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load and restrict data\n",
    "pivot_df = pd.read_csv(\"synthetic_temp_polling_data.csv\").head(20000)\n",
    "\n",
    "# Parameters\n",
    "M = 2  # Maximum nodes polled simultaneously\n",
    "aoii_penalty = 0.01\n",
    "beta_1 = 0.99\n",
    "beta_2 = 0.99\n",
    "\n",
    "columns = [col for col in pivot_df.columns if col != \"SN\"]\n",
    "\n",
    "# Initialize tracking variables with reasonable defaults\n",
    "# Pre-compute reasonable min/max values from the first 100 rows to avoid extreme normalization\n",
    "initial_data = pivot_df.head(100)\n",
    "global_min = initial_data[columns].min().min()\n",
    "global_max = initial_data[columns].max().max()\n",
    "\n",
    "# Use these values as initial bounds to avoid infinity\n",
    "last_update_times = {col: 0 for col in columns}\n",
    "state_node = {col: np.array([20, 0.1]) for col in columns}  # Initialized as normalized\n",
    "value_min_max_history = {col: {'min': global_min, 'max': global_max} for col in columns}\n",
    "category_counts = {'Category A': 0, 'Category B': 0}\n",
    "\n",
    "# Initialize prediction and true value storage for MSE calculation\n",
    "predictions = {col: [] for col in columns}\n",
    "true_values = {col: [] for col in columns}\n",
    "true_values_normalized = {col: [] for col in columns}\n",
    "\n",
    "# Functions\n",
    "def calculate_aoii_sink(current_time, last_received_time, normalised_rate_of_change):\n",
    "    return abs((current_time - last_received_time) * normalised_rate_of_change)\n",
    "\n",
    "def update_node_state_dewma(norm_measured_value, last_state_value, last_rate_of_change, delta_t, beta_1, beta_2):\n",
    "    # Handle edge cases for delta_t\n",
    "    if delta_t < 1:\n",
    "        delta_t = 1  # Minimum value to avoid division by zero\n",
    "        \n",
    "    x1 = beta_1 * norm_measured_value + (1 - beta_1) * (last_state_value + last_rate_of_change * delta_t)\n",
    "    x2 = beta_2 * (x1 - last_state_value) / delta_t + (1 - beta_2) * last_rate_of_change\n",
    "    \n",
    "    # Add bounds check to prevent extreme values\n",
    "    if np.isnan(x1) or np.isnan(x2):\n",
    "        return last_state_value, last_rate_of_change\n",
    "    \n",
    "    return x1, x2\n",
    "\n",
    "def extract_node_id(col_name):\n",
    "    digits = ''.join(filter(str.isdigit, col_name))\n",
    "    return int(digits) if digits else None\n",
    "\n",
    "def get_node_category(node_id):\n",
    "    if node_id is None:\n",
    "        return None\n",
    "    if 1 <= node_id <= 5:\n",
    "        return 'Category A'\n",
    "    elif 6 <= node_id <= 10:\n",
    "        return 'Category B'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def normalize_value(value, min_val, max_val):\n",
    "    \"\"\"Safely normalize a value with proper bounds checking\"\"\"\n",
    "    epsilon = 1e-6\n",
    "    range_val = max_val - min_val\n",
    "    \n",
    "    if range_val > epsilon:\n",
    "        norm_val = (value - min_val) / range_val\n",
    "        # Clamp to [0,1] to prevent any out-of-bounds issues\n",
    "        return max(0, min(1, norm_val))\n",
    "    else:\n",
    "        return 0.5  # Default to middle value if no meaningful range\n",
    "\n",
    "# Debugging counters\n",
    "nan_prediction_count = 0\n",
    "nan_true_val_count = 0\n",
    "\n",
    "# Main simulation loop\n",
    "for t in range(len(pivot_df)):\n",
    "    whittle_indices = {}\n",
    "\n",
    "    for col in columns:\n",
    "        last_state_value, last_rate_of_change = state_node[col]\n",
    "        delta_t_dynamic = max(1, t - last_update_times[col])  # Ensure at least 1 to avoid division issues\n",
    "\n",
    "        # Estimate the normalised state at sink (without new measurement)\n",
    "        estimated_norm_value = last_state_value + last_rate_of_change * delta_t_dynamic\n",
    "        normalised_rate_of_change = abs(last_rate_of_change)\n",
    "\n",
    "        # Store the true value for MSE calculation\n",
    "        true_val = pivot_df.loc[t, col]\n",
    "        true_values[col].append(true_val)\n",
    "        \n",
    "        # Update min/max for the current timestep\n",
    "        current_min = value_min_max_history[col]['min']\n",
    "        current_max = value_min_max_history[col]['max']\n",
    "        \n",
    "        value_min_max_history[col]['min'] = min(current_min, true_val)\n",
    "        value_min_max_history[col]['max'] = max(current_max, true_val)\n",
    "        \n",
    "        # Use normalized value function for safety\n",
    "        norm_true_val = normalize_value(true_val, current_min, current_max)\n",
    "        true_values_normalized[col].append(norm_true_val)\n",
    "        \n",
    "        # Ensure prediction is in valid range [0,1]\n",
    "        norm_prediction = max(0, min(1, estimated_norm_value))\n",
    "        predictions[col].append(norm_prediction)\n",
    "        \n",
    "        # Check for NaN values for debugging\n",
    "        if np.isnan(norm_prediction):\n",
    "            nan_prediction_count += 1\n",
    "        if np.isnan(norm_true_val):\n",
    "            nan_true_val_count += 1\n",
    "        \n",
    "        # Calculate AOII\n",
    "        current_aoii = calculate_aoii_sink(t, last_update_times[col], normalised_rate_of_change)\n",
    "        future_aoii_passive = calculate_aoii_sink(t + 1, last_update_times[col], normalised_rate_of_change)\n",
    "        future_aoii_active = 0\n",
    "\n",
    "        q_passive = current_aoii + future_aoii_passive\n",
    "        q_active = current_aoii + future_aoii_active + aoii_penalty\n",
    "        whittle_indices[col] = q_passive - q_active\n",
    "\n",
    "    # Poll decision\n",
    "    nodes_to_poll = [col for col in whittle_indices if whittle_indices[col] >= 0]\n",
    "    if len(nodes_to_poll) > M:\n",
    "        nodes_to_poll = sorted(nodes_to_poll, key=whittle_indices.get, reverse=True)[:M]\n",
    "\n",
    "    for col in nodes_to_poll:\n",
    "        measured_value = pivot_df.loc[t, col]\n",
    "\n",
    "        # Dynamically update min/max and normalize measured value\n",
    "        min_val = value_min_max_history[col]['min']\n",
    "        max_val = value_min_max_history[col]['max']\n",
    "\n",
    "        # Use the normalization function\n",
    "        normalised_measurement = normalize_value(measured_value, min_val, max_val)\n",
    "\n",
    "        last_state_value, last_rate_of_change = state_node[col]\n",
    "        delta_t_dynamic = max(1, t - last_update_times[col])  # Ensure at least 1 \n",
    "\n",
    "        # Update state using normalised measurement\n",
    "        new_state = update_node_state_dewma(\n",
    "            normalised_measurement, last_state_value, last_rate_of_change, delta_t_dynamic, beta_1, beta_2\n",
    "        )\n",
    "        \n",
    "        # Check for NaN in new state\n",
    "        if np.isnan(new_state[0]) or np.isnan(new_state[1]):\n",
    "            # Keep previous state if new state has NaN\n",
    "            pass\n",
    "        else:\n",
    "            state_node[col] = new_state\n",
    "            \n",
    "        last_update_times[col] = t\n",
    "\n",
    "        # Track polling count by category\n",
    "        node_id = extract_node_id(col)\n",
    "        if node_id:\n",
    "            if 1 <= node_id <= 5:\n",
    "                category_counts['Category A'] += 1\n",
    "            elif 6 <= node_id <= 10:\n",
    "                category_counts['Category B'] += 1\n",
    "\n",
    "# Print debugging information\n",
    "if nan_prediction_count > 0 or nan_true_val_count > 0:\n",
    "    print(f\"Warning: Found {nan_prediction_count} NaN predictions and {nan_true_val_count} NaN true values during simulation\")\n",
    "\n",
    "# Display polling counts\n",
    "print(\"Transmission Count by Category:\")\n",
    "for category, count in category_counts.items():\n",
    "    print(f\"{category}: {count} times\")\n",
    "\n",
    "# Calculate MSE for each node - handling NaN values\n",
    "node_mse = {}\n",
    "for col in columns:\n",
    "    # Filter out NaN values before calculating MSE\n",
    "    pred_array = np.array(predictions[col])\n",
    "    true_array = np.array(true_values_normalized[col])\n",
    "    \n",
    "    # Create mask for non-NaN values in both arrays\n",
    "    valid_mask = ~np.isnan(pred_array) & ~np.isnan(true_array)\n",
    "    \n",
    "    # Only calculate MSE with valid values\n",
    "    if np.any(valid_mask):\n",
    "        node_mse[col] = mean_squared_error(\n",
    "            true_array[valid_mask], \n",
    "            pred_array[valid_mask]\n",
    "        )\n",
    "    else:\n",
    "        node_mse[col] = np.nan  # If all values are NaN\n",
    "\n",
    "# Calculate total MSE - handling NaN values\n",
    "all_true_normalized = []\n",
    "all_predictions = []\n",
    "for col in columns:\n",
    "    pred_array = np.array(predictions[col])\n",
    "    true_array = np.array(true_values_normalized[col])\n",
    "    \n",
    "    # Create mask for non-NaN values\n",
    "    valid_mask = ~np.isnan(pred_array) & ~np.isnan(true_array)\n",
    "    \n",
    "    # Only add valid values to the overall arrays\n",
    "    if np.any(valid_mask):\n",
    "        all_true_normalized.extend(true_array[valid_mask])\n",
    "        all_predictions.extend(pred_array[valid_mask])\n",
    "\n",
    "if all_true_normalized and all_predictions:\n",
    "    total_mse = mean_squared_error(all_true_normalized, all_predictions)\n",
    "    print(f\"\\nTotal MSE: {total_mse:.6f}\")\n",
    "else:\n",
    "    print(\"\\nUnable to calculate Total MSE: No valid data points\")\n",
    "\n",
    "# Calculate MSE per category\n",
    "category_mse = {'Category A': [], 'Category B': []}\n",
    "for col in columns:\n",
    "    node_id = extract_node_id(col)\n",
    "    category = get_node_category(node_id)\n",
    "    if category and not np.isnan(node_mse[col]):\n",
    "        category_mse[category].append(node_mse[col])\n",
    "\n",
    "# Print MSE per category - handling NaN values\n",
    "print(\"\\nMSE by Category:\")\n",
    "for category, mse_values in category_mse.items():\n",
    "    # Filter out NaN values\n",
    "    valid_mse = [val for val in mse_values if not np.isnan(val)]\n",
    "    if valid_mse:\n",
    "        avg_mse = np.mean(valid_mse)\n",
    "        print(f\"{category}: {avg_mse:.6f}\")\n",
    "    else:\n",
    "        print(f\"{category}: No valid data for MSE calculation\")\n",
    "\n",
    "# Simply print the results without plotting\n",
    "print(\"\\n--- Detailed MSE Results ---\")\n",
    "print(\"Total nodes analyzed:\", len(columns))\n",
    "\n",
    "# Calculate average MSE across all nodes, handling NaN values\n",
    "valid_mse_values = [val for val in node_mse.values() if not np.isnan(val)]\n",
    "if valid_mse_values:\n",
    "    avg_node_mse = np.mean(valid_mse_values)\n",
    "    print(f\"Average MSE across all nodes: {avg_node_mse:.6f}\")\n",
    "else:\n",
    "    print(\"Average MSE across all nodes: Unable to calculate (all values are NaN)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transmission Count by Category:\n",
      "Category A: 47 times\n",
      "Category B: 61 times\n",
      "Category C: 0 times\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and restrict data\n",
    "pivot_df = pd.read_csv(\"synthetic_temp_polling_data.csv\").head(20000)\n",
    "\n",
    "# Parameters\n",
    "M = 2  # Maximum nodes polled simultaneously\n",
    "aoii_penalty = 0.5\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.9\n",
    "\n",
    "columns = [col for col in pivot_df.columns if col != \"SN\"]\n",
    "\n",
    "# Initialize tracking variables\n",
    "last_update_times = {col: 0 for col in columns}\n",
    "state_node = {col: np.array([20, 0.1]) for col in columns}  # Initialised in normalised scale\n",
    "value_min_max_history = {col: {'min': float('inf'), 'max': float('-inf')} for col in columns}\n",
    "category_counts = {'Category A': 0, 'Category B': 0, 'Category C': 0}\n",
    "\n",
    "# Functions\n",
    "def calculate_aoii_sink(current_time, last_received_time, normalised_rate_of_change):\n",
    "    return abs((current_time - last_received_time) * normalised_rate_of_change)\n",
    "\n",
    "def update_node_state_dewma(normalised_measured_value, normalised_last_state_value, last_rate_of_change, delta_t, beta_1, beta_2):\n",
    "    x1 = beta_1 * normalised_measured_value + (1 - beta_1) * (normalised_last_state_value + last_rate_of_change * delta_t)\n",
    "    x2 = beta_2 * (x1 - last_state_value) / delta_t + (1 - beta_2) * last_rate_of_change\n",
    "    return x1, x2\n",
    "\n",
    "def extract_node_id(col_name):\n",
    "    digits = ''.join(filter(str.isdigit, col_name))\n",
    "    return int(digits) if digits else None\n",
    "\n",
    "# Main simulation loop\n",
    "for t in range(len(pivot_df)):\n",
    "    whittle_indices = {}\n",
    "\n",
    "    for col in columns:\n",
    "        last_state_value, last_rate_of_change = state_node[col]\n",
    "        delta_t_dynamic = t - last_update_times[col]\n",
    "\n",
    "        # Estimate normalised value at sink without new measurement\n",
    "        estimated_norm_value = last_state_value + last_rate_of_change * delta_t_dynamic\n",
    "        normalised_rate_of_change = abs(last_rate_of_change)\n",
    "\n",
    "        current_aoii = calculate_aoii_sink(t, last_update_times[col], normalised_rate_of_change)\n",
    "        future_aoii_passive = calculate_aoii_sink(t + 1, last_update_times[col], normalised_rate_of_change)\n",
    "        future_aoii_active = 0\n",
    "\n",
    "        q_passive = current_aoii + future_aoii_passive\n",
    "        q_active = current_aoii + future_aoii_active + aoii_penalty\n",
    "        whittle_indices[col] = q_passive - q_active\n",
    "\n",
    "    # Poll decision\n",
    "    nodes_to_poll = [col for col in whittle_indices if whittle_indices[col] >= 0]\n",
    "    if len(nodes_to_poll) > M:\n",
    "        nodes_to_poll = sorted(nodes_to_poll, key=whittle_indices.get, reverse=True)[:M]\n",
    "\n",
    "    for col in nodes_to_poll:\n",
    "        measured_value = pivot_df.loc[t, col]\n",
    "\n",
    "        # Update min/max and normalise measured value upon receiving\n",
    "        value_min_max_history[col]['min'] = min(value_min_max_history[col]['min'], measured_value)\n",
    "        value_min_max_history[col]['max'] = max(value_min_max_history[col]['max'], measured_value)\n",
    "        min_val = value_min_max_history[col]['min']\n",
    "        max_val = value_min_max_history[col]['max']\n",
    "\n",
    "        epsilon = 1e-6\n",
    "        normalised_measurement = (measured_value - min_val) / (max_val - min_val + epsilon)\n",
    "\n",
    "        last_state_value, last_rate_of_change = state_node[col]\n",
    "        delta_t_dynamic = t - last_update_times[col]\n",
    "\n",
    "        # Update state using normalised values\n",
    "        state_node[col] = update_node_state_dewma(\n",
    "            normalised_measurement, last_state_value, last_rate_of_change, delta_t_dynamic, beta_1, beta_2\n",
    "        )\n",
    "        last_update_times[col] = t\n",
    "\n",
    "        # Track polling count by category\n",
    "        node_id = extract_node_id(col)\n",
    "        if node_id:\n",
    "            if 1 <= node_id <= 5:\n",
    "                category_counts['Category A'] += 1\n",
    "            elif 6 <= node_id <= 10:\n",
    "                category_counts['Category B'] += 1\n",
    "            elif 21 <= node_id <= 30:\n",
    "                category_counts['Category C'] += 1\n",
    "\n",
    "# Display polling counts\n",
    "print(\"Transmission Count by Category:\")\n",
    "for category, count in category_counts.items():\n",
    "    print(f\"{category}: {count} times\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running normalized simulation...\n",
      "Step 0/20000...\n",
      "Step 5000/20000...\n",
      "Step 10000/20000...\n",
      "Step 15000/20000...\n",
      "\n",
      "RESULTS:\n",
      "Category A: 28 polls\n",
      "Category B: 20 polls\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "pivot_df = pd.read_csv(\"synthetic_temp_polling_data.csv\")\n",
    "pivot_df = pivot_df.head(20000)  # Restrict to first 20000 time steps\n",
    "\n",
    "M = 2  # Maximum number of nodes that can be polled\n",
    "aoii_penalty = 0.5\n",
    "initial_value = 20  # Initial estimate for last polled values\n",
    "\n",
    "# Set parameters\n",
    "beta_1 = 0.9  # dEWMA parameter for state value\n",
    "beta_2 = 0.9  # dEWMA parameter for rate of change\n",
    "\n",
    "# Extract column names, ensuring \"SN\" is excluded\n",
    "columns = [col for col in pivot_df.columns if col != \"SN\"]\n",
    "\n",
    "# Function to calculate Age of Incorrect Information (AoII) at the sink\n",
    "def calculate_aoii_sink(current_time, last_received_time, last_rate_of_change):\n",
    "    return abs((current_time - last_received_time) * last_rate_of_change)\n",
    "\n",
    "# Helper function to update state using dEWMA\n",
    "def update_node_state_dewma(measured_value, last_state_value, last_rate_of_change, delta_t, beta_1, beta_2):\n",
    "    x1 = beta_1 * measured_value + (1 - beta_1) * (last_state_value + last_rate_of_change * delta_t)\n",
    "    x2 = beta_2 * (x1 - last_state_value) / delta_t + (1 - beta_2) * last_rate_of_change\n",
    "    return x1, x2\n",
    "\n",
    "# Function to extract numeric node ID from column names\n",
    "def extract_node_id(col_name):\n",
    "    digits = ''.join(filter(str.isdigit, col_name))\n",
    "    return int(digits) if digits else None\n",
    "\n",
    "# Normalize a value using fixed min-max range for each category\n",
    "def normalize_value(value, node_id):\n",
    "    # Fixed ranges per category\n",
    "    if 1 <= node_id <= 5:  # Category A\n",
    "        min_val = 15\n",
    "        max_val = 25\n",
    "    elif 6 <= node_id <= 10:  # Category B\n",
    "        min_val = 75\n",
    "        max_val = 125\n",
    "    else:\n",
    "        # Default fallback\n",
    "        return value / 100.0\n",
    "    \n",
    "    # Standard min-max normalization with clipping\n",
    "    normalized = (value - min_val) / (max_val - min_val)\n",
    "    normalized = max(0, min(1, normalized))\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "# Run normalized simulation\n",
    "def run_normalized_simulation():\n",
    "    # Initialize tracking variables\n",
    "    last_update_times = {col: 0 for col in columns}\n",
    "    state_node = {}\n",
    "    for col in columns:\n",
    "        node_id = extract_node_id(col)\n",
    "        if node_id is not None:\n",
    "            # Get initial value from dataset or use default\n",
    "            initial_val = pivot_df.loc[0, col] if not pd.isna(pivot_df.loc[0, col]) else initial_value\n",
    "            normalized_val = normalize_value(initial_val, node_id)\n",
    "            state_node[col] = np.array([normalized_val, 0.1])\n",
    "        else:\n",
    "            state_node[col] = np.array([0.5, 0.1])\n",
    "    \n",
    "    # Track category counts\n",
    "    category_counts = {'Category A': 0, 'Category B': 0}\n",
    "\n",
    "    for t in range(len(pivot_df)):\n",
    "        # Progress indicator (every 5000 steps)\n",
    "        if t % 5000 == 0:\n",
    "            print(f\"Step {t}/{len(pivot_df)}...\")\n",
    "            \n",
    "        # Compute Whittle indices for each node\n",
    "        whittle_indices = {}\n",
    "        for col in columns:\n",
    "            node_id = extract_node_id(col)\n",
    "            if node_id is None:\n",
    "                continue\n",
    "                \n",
    "            last_state_value, last_rate_of_change = state_node[col]\n",
    "            measured_value = pivot_df.loc[t, col]\n",
    "            \n",
    "            # Skip if value is NaN\n",
    "            if pd.isna(measured_value):\n",
    "                continue\n",
    "\n",
    "            # Calculate AoII in normalized space\n",
    "            current_aoii = calculate_aoii_sink(t, last_update_times[col], last_rate_of_change)\n",
    "            future_aoii_passive = calculate_aoii_sink(t+1, last_update_times[col], last_rate_of_change)\n",
    "            future_aoii_active = 0  # AoII resets to 0 if polled\n",
    "\n",
    "            # Whittle index calculations\n",
    "            q_passive = current_aoii + future_aoii_passive\n",
    "            q_active = current_aoii + future_aoii_active + aoii_penalty\n",
    "            whittle_indices[col] = q_passive - q_active\n",
    "\n",
    "        # Select nodes to poll based on Whittle indices\n",
    "        nodes_to_poll = [col for col in whittle_indices if whittle_indices[col] >= 0]\n",
    "        if len(nodes_to_poll) > M:\n",
    "            nodes_to_poll = sorted(nodes_to_poll, key=whittle_indices.get, reverse=True)[:M]\n",
    "            \n",
    "        # Poll selected nodes and update states\n",
    "        for col in nodes_to_poll:\n",
    "            measured_value = pivot_df.loc[t, col]\n",
    "            \n",
    "            # Skip if value is NaN\n",
    "            if pd.isna(measured_value):\n",
    "                continue\n",
    "            \n",
    "            node_id = extract_node_id(col)\n",
    "            if node_id is None:\n",
    "                continue\n",
    "                \n",
    "            # Normalize the measured value\n",
    "            normalized_value = normalize_value(measured_value, node_id)\n",
    "            \n",
    "            # Get last state in normalized space\n",
    "            last_state_value, last_rate_of_change = state_node[col]\n",
    "            \n",
    "            # Calculate time since last update (min 1 to avoid division by zero)\n",
    "            delta_t_dynamic = max(1, t - last_update_times[col])\n",
    "            \n",
    "            # Update state using dEWMA with normalized measured value\n",
    "            state_node[col] = update_node_state_dewma(\n",
    "                normalized_value, last_state_value, last_rate_of_change, \n",
    "                delta_t_dynamic, beta_1, beta_2\n",
    "            )\n",
    "            \n",
    "            last_update_times[col] = t\n",
    "\n",
    "            # Categorize based on node ID\n",
    "            if 1 <= node_id <= 5:\n",
    "                category_counts['Category A'] += 1\n",
    "            elif 6 <= node_id <= 10:\n",
    "                category_counts['Category B'] += 1\n",
    "\n",
    "    return category_counts\n",
    "\n",
    "# Run the simulation and print results\n",
    "print(\"Running normalized simulation...\")\n",
    "category_counts = run_normalized_simulation()\n",
    "\n",
    "print(\"\\nRESULTS:\")\n",
    "print(f\"Category A: {category_counts['Category A']} polls\")\n",
    "print(f\"Category B: {category_counts['Category B']} polls\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
