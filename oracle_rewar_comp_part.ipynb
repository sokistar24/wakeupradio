{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'M' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 82\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cumulative_rewards\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Run the simulation\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m cumulative_rewards_whittle \u001b[38;5;241m=\u001b[39m run_simulation_whittle_aoii(pivot_df, columns, \u001b[43mM\u001b[49m, theta, penalty, aoii_penalty)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Save cumulative rewards to CSV\u001b[39;00m\n\u001b[0;32m     85\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(cumulative_rewards_whittle, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcumulative_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcumulative_rewards_whittle_aoii.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'M' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "pivot_df = pd.read_csv('synthetic_scenario_three.csv')\n",
    "# Set parameters\n",
    "penalty = -0.5  # Penalty for polling when difference <= theta\n",
    "aoii_penalty = 0.5\n",
    "theta = 0.5  # Threshold for reward\n",
    "beta_1 = 0.3  # dEWMA parameter for state value\n",
    "beta_2 = 0.3  # dEWMA parameter for rate of change\n",
    "columns = pivot_df.columns  # Get the column names dynamically\n",
    "num_nodes = len(columns)  # Number of nodes based on dataset columns\n",
    "num_time_steps = len(pivot_df)  # Total time steps based on dataset length\n",
    "\n",
    "# Function to calculate Age of Incorrect Information (AoII) at the sink\n",
    "def calculate_aoii_sink(current_time, last_received_time, last_rate_of_change):\n",
    "    return abs((current_time - last_received_time) * last_rate_of_change)\n",
    "\n",
    "# Helper function to update state using dEWMA\n",
    "def update_node_state_dewma(measured_value, last_state_value, last_rate_of_change, delta_t, beta_1, beta_2):\n",
    "    x1 = beta_1 * measured_value + (1 - beta_1) * (last_state_value + last_rate_of_change * delta_t)\n",
    "    x2 = beta_2 * (x1 - last_state_value) / delta_t + (1 - beta_2) * last_rate_of_change\n",
    "    return x1, x2\n",
    "\n",
    "# Helper function to calculate reward\n",
    "def calculate_reward(measured_value, last_state_value, theta, penalty):\n",
    "    if abs(measured_value - last_state_value) > theta:\n",
    "        return 1  # Reward\n",
    "    return penalty  # Penalty\n",
    "\n",
    "# Main function to simulate Whittle AoII with rewards\n",
    "def run_simulation_whittle_aoii(pivot_df, columns, M, theta, penalty, aoii_penalty):\n",
    "    cumulative_reward = 0  # Track total cumulative reward\n",
    "    cumulative_rewards = []  # Store cumulative average reward over time\n",
    "    last_update_times = {col: 0 for col in columns}  # Last update time for each node\n",
    "    state_node = {col: np.array([20.0, 0.1]) for col in columns}  # Node states\n",
    "\n",
    "    for t in range(len(pivot_df)):\n",
    "        # Step 1: Compute Whittle indices for each node based on AoII\n",
    "        whittle_indices = {}\n",
    "        for col in columns:\n",
    "            delta_t_dynamic = t - last_update_times[col]  # Time since last update\n",
    "            last_state_value, last_rate_of_change = state_node[col]\n",
    "            measured_value = pivot_df.loc[t, col]\n",
    "\n",
    "            # Correct AoII calculation at the sink using rate of change\n",
    "            current_aoii = calculate_aoii_sink(t, last_update_times[col], last_rate_of_change)\n",
    "            future_aoii_passive = current_aoii + delta_t_dynamic  # Updated AoII increase assumption for passive\n",
    "            future_aoii_active = 0  # AoII resets to 0 if polled\n",
    "\n",
    "            # Whittle index calculations\n",
    "            q_passive = current_aoii + future_aoii_passive\n",
    "            q_active = current_aoii + future_aoii_active + aoii_penalty\n",
    "            whittle_indices[col] = q_passive - q_active\n",
    "\n",
    "        # Step 2: Select top M nodes to poll based on Whittle indices\n",
    "        nodes_to_poll = [col for col in whittle_indices if whittle_indices[col] >= 0]\n",
    "        if len(nodes_to_poll) > M:\n",
    "            nodes_to_poll = sorted(nodes_to_poll, key=whittle_indices.get, reverse=True)[:M]\n",
    "\n",
    "        # Step 3: Poll selected nodes and calculate rewards\n",
    "        for col in nodes_to_poll:\n",
    "            measured_value = pivot_df.loc[t, col]\n",
    "            last_state_value, last_rate_of_change = state_node[col]\n",
    "\n",
    "            # Calculate reward after polling\n",
    "            reward = calculate_reward(measured_value, last_state_value, theta, penalty)\n",
    "            cumulative_reward += reward  # Update cumulative reward\n",
    "\n",
    "            # Update node state and last update time\n",
    "            state_node[col] = update_node_state_dewma(\n",
    "                measured_value, last_state_value, last_rate_of_change, delta_t=1, beta_1=beta_1, beta_2=beta_2\n",
    "            )\n",
    "            last_update_times[col] = t\n",
    "\n",
    "        # Step 4: Calculate cumulative average reward\n",
    "        cumulative_rewards.append(cumulative_reward / (t + 1))\n",
    "\n",
    "    return cumulative_rewards\n",
    "\n",
    "# Run the simulation\n",
    "cumulative_rewards_whittle = run_simulation_whittle_aoii(pivot_df, columns, M, theta, penalty, aoii_penalty)\n",
    "\n",
    "# Save cumulative rewards to CSV\n",
    "pd.DataFrame(cumulative_rewards_whittle, columns=[\"cumulative_reward\"]).to_csv('cumulative_rewards_whittle_aoii.csv', index=False)\n",
    "\n",
    "# Plot cumulative average reward over time\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(cumulative_rewards_whittle) + 1), cumulative_rewards_whittle, label='Whittle AoII Cumulative Average Reward')\n",
    "plt.xlabel('Time Steps', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Cumulative Average Reward', fontsize=14, fontweight='bold')\n",
    "plt.title('Whittle AoII Cumulative Average Reward over Time', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.xlim(0, len(cumulative_rewards_whittle))\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('whittle_aoii_cumulative_average_reward.png')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation_with_rate_of_change_logging(pivot_df, num_nodes, M, theta, penalty, covariance_threshold):\n",
    "    \"\"\"Runs the simulation using Kalman filter (1D) with two-step pooling and logs rate of change.\"\"\"\n",
    "    \n",
    "    state_estimates = {f'mote{i}': np.array([[20], [0.1]]) for i in range(1, num_nodes + 1)}\n",
    "    P = {f'mote{i}': np.zeros((2, 2)) for i in range(1, num_nodes + 1)}  # Initialize covariance\n",
    "    last_observations = {f'mote{i}': 0.0 for i in range(1, num_nodes + 1)}  # Last observed values\n",
    "    last_update_times = {f'mote{i}': 0 for i in range(1, num_nodes + 1)}  # Last measurement times\n",
    "    \n",
    "    cumulative_reward = 0\n",
    "    cumulative_rewards = []  # To store cumulative rewards over time\n",
    "    rate_of_change_log = []  # Log for rate of change\n",
    "\n",
    "    def predict_node_state(x_hat, delta_t):\n",
    "        A = get_state_transition_matrix(delta_t)\n",
    "        return A @ x_hat\n",
    "\n",
    "    for idx, row in pivot_df.iterrows():\n",
    "        current_time_step = idx\n",
    "\n",
    "        # Print rate of change for the first 20 time steps\n",
    "        if current_time_step < 20:\n",
    "            for mote, state in state_estimates.items():\n",
    "                rate_of_change = state[1, 0]  # Extract rate of change from the state\n",
    "                print(f\"Time Step {current_time_step}, Node {mote}, Rate of Change: {rate_of_change}\")\n",
    "\n",
    "        # Step 1: Select nodes based on highest covariance\n",
    "        covariance_traces = {mote: np.trace(P[mote]) for mote in state_estimates}\n",
    "        sorted_motes_by_trace = sorted(covariance_traces, key=covariance_traces.get, reverse=True)\n",
    "        top_covariance_nodes = [mote for mote in sorted_motes_by_trace if covariance_traces[mote] > covariance_threshold][:M]\n",
    "\n",
    "        # Step 2: Filter nodes with significant prediction error\n",
    "        nodes_to_poll = []\n",
    "        for mote in top_covariance_nodes:\n",
    "            column_name = mote\n",
    "            if column_name in row:\n",
    "                delta_t = max(current_time_step - last_update_times[column_name], 1)\n",
    "                predicted_state = predict_node_state(state_estimates[column_name], delta_t)\n",
    "                last_observed_value = last_observations[column_name]\n",
    "                prediction_error = abs(last_observed_value - predicted_state[0, 0])\n",
    "\n",
    "                # Include node only if prediction error exceeds threshold\n",
    "                if prediction_error > theta:\n",
    "                    nodes_to_poll.append(mote)\n",
    "\n",
    "        total_reward_for_round = 0\n",
    "\n",
    "        # Process selected nodes for reward calculation\n",
    "        for mote in nodes_to_poll:\n",
    "            column_name = mote\n",
    "            measured_value = row[column_name]  # Current measurement\n",
    "            last_observed_value = last_observations[column_name]  # Last observed value\n",
    "\n",
    "            # Reward or penalty based on actual measurement\n",
    "            if abs(last_observed_value - measured_value) > theta:\n",
    "                total_reward_for_round += 1  # Reward\n",
    "            else:\n",
    "                total_reward_for_round += penalty  # Penalty\n",
    "\n",
    "            # Update Kalman filter\n",
    "            previous_state = state_estimates[column_name]\n",
    "            delta_t = max(current_time_step - last_update_times[column_name], 1)\n",
    "            A = get_state_transition_matrix(delta_t)\n",
    "            Pp = A @ P[column_name] @ A.T + Q\n",
    "            xp = A @ previous_state\n",
    "            z = np.array([[measured_value]])\n",
    "            K = Pp @ H.T @ np.linalg.inv(H @ Pp @ H.T + R)\n",
    "            x_hat = xp + K @ (z - H @ xp)\n",
    "            P_hat = Pp - K @ H @ Pp\n",
    "\n",
    "            # Update state estimates and covariance\n",
    "            state_estimates[column_name] = x_hat\n",
    "            P[column_name] = P_hat\n",
    "            last_observations[column_name] = measured_value\n",
    "            last_update_times[column_name] = current_time_step\n",
    "\n",
    "        # Update cumulative rewards\n",
    "        cumulative_reward += total_reward_for_round\n",
    "        cumulative_rewards.append(cumulative_reward / (idx + 1))\n",
    "\n",
    "    return cumulative_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = np.array([[1, 0]])                # Measurement matrix\n",
    "Q = np.array([[1e-5, 0], [0, 1e-5]])  # Process noise covariance\n",
    "R = np.array([[0.5]])                 # Measurement noise covariance\n",
    "\n",
    "def get_state_transition_matrix(delta_t):\n",
    "    return np.array([[1, delta_t], [0, 1]])\n",
    "\n",
    "def run_simulation_with_two_step_pooling(pivot_df, num_nodes, M, theta, penalty, covariance_threshold):\n",
    "    \"\"\"Runs the simulation using Kalman filter (1D) with two-step pooling.\"\"\"\n",
    "    \n",
    "    state_estimates = {f'mote{i}': np.array([[20], [0.1]]) for i in range(1, num_nodes + 1)}\n",
    "    P = {f'mote{i}': np.zeros((2, 2)) for i in range(1, num_nodes + 1)}  # Initialize covariance\n",
    "    last_update_times = {f'mote{i}': 0 for i in range(1, num_nodes + 1)}  # Last measurement times\n",
    "    \n",
    "    cumulative_reward = 0\n",
    "    cumulative_rewards = []  # To store cumulative rewards over time\n",
    "    polled_nodes_info = []  # Log for selected nodes and their states\n",
    "\n",
    "    for idx, row in pivot_df.iterrows():\n",
    "        current_time_step = idx\n",
    "\n",
    "        # Step 1: Select nodes based on highest covariance\n",
    "        covariance_traces = {mote: np.trace(P[mote]) for mote in state_estimates}\n",
    "        sorted_motes_by_trace = sorted(covariance_traces, key=covariance_traces.get, reverse=True)\n",
    "        top_covariance_nodes = [mote for mote in sorted_motes_by_trace if covariance_traces[mote] > covariance_threshold][:M]\n",
    "\n",
    "        # Step 2: Filter nodes based on the rate of change\n",
    "        nodes_to_poll = []\n",
    "        for mote in top_covariance_nodes:\n",
    "            column_name = mote\n",
    "            if column_name in row:\n",
    "                delta_t = max(current_time_step - last_update_times[column_name], 1)\n",
    "                previous_state = state_estimates[column_name]\n",
    "                rate_of_change = abs(previous_state[1, 0])  # Extract rate of change\n",
    "                projected_change = rate_of_change * delta_t\n",
    "\n",
    "                # Include node only if projected change exceeds the threshold\n",
    "                if projected_change > theta:\n",
    "                    nodes_to_poll.append(mote)\n",
    "                    polled_nodes_info.append({\n",
    "                        \"time_step\": current_time_step,\n",
    "                        \"node\": mote,\n",
    "                        \"projected_change\": projected_change,\n",
    "                        \"covariance_trace\": covariance_traces[mote]\n",
    "                    })\n",
    "\n",
    "        total_reward_for_round = 0\n",
    "\n",
    "        # Process selected nodes for reward calculation\n",
    "        for mote in nodes_to_poll:\n",
    "            column_name = mote\n",
    "            measured_value = row[column_name]  # Current measurement\n",
    "\n",
    "            # Reward or penalty based on the actual measurement\n",
    "            if abs(state_estimates[column_name][0, 0] - measured_value) > theta:\n",
    "                total_reward_for_round += 1  # Reward\n",
    "            else:\n",
    "                total_reward_for_round += penalty  # Penalty\n",
    "\n",
    "            # Update Kalman filter\n",
    "            previous_state = state_estimates[column_name]\n",
    "            delta_t = max(current_time_step - last_update_times[column_name], 1)\n",
    "            A = get_state_transition_matrix(delta_t)\n",
    "            Pp = A @ P[column_name] @ A.T + Q\n",
    "            xp = A @ previous_state\n",
    "            z = np.array([[measured_value]])\n",
    "            K = Pp @ H.T @ np.linalg.inv(H @ Pp @ H.T + R)\n",
    "            x_hat = xp + K @ (z - H @ xp)\n",
    "            P_hat = Pp - K @ H @ Pp\n",
    "\n",
    "            # Update state estimates and covariance\n",
    "            state_estimates[column_name] = x_hat\n",
    "            P[column_name] = P_hat\n",
    "            last_update_times[column_name] = current_time_step\n",
    "\n",
    "        # Update cumulative rewards\n",
    "        cumulative_reward += total_reward_for_round\n",
    "        cumulative_rewards.append(cumulative_reward / (idx + 1))\n",
    "\n",
    "    # Save selected nodes information to CSV\n",
    "    pd.DataFrame(polled_nodes_info).to_csv('polled_nodes_two_step_pooling.csv', index=False)\n",
    "\n",
    "    return cumulative_rewards, pd.DataFrame(polled_nodes_info)\n",
    "\n",
    "# Run simulation\n",
    "cumulative_rewards, polled_nodes_info = run_simulation_with_two_step_pooling(\n",
    "    pivot_df, num_nodes=50, M=10, theta=0.1, penalty=-0.5, covariance_threshold=0.5\n",
    ")\n",
    "\n",
    "# Save cumulative rewards to CSV\n",
    "pd.DataFrame(cumulative_rewards, columns=[\"cumulative_reward\"]).to_csv('cumulative_rewards_two_step.csv', index=False)\n",
    "\n",
    "# Plot cumulative average reward\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(cumulative_rewards) + 1), cumulative_rewards, label='Two-Step Pooling Reward')\n",
    "plt.xlabel('Time Steps', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Cumulative Average Reward', fontsize=14, fontweight='bold')\n",
    "plt.title('Two-Step Pooling - Cumulative Average Reward', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.xlim(0, len(cumulative_rewards))\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('two_step_pooling_reward.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation_kalman_with_covariance_check(pivot_df, num_nodes, M, theta, penalty, covariance_threshold):\n",
    "    \"\"\"Runs the simulation using Kalman filter (1D) and computes cumulative average reward with covariance threshold check.\"\"\"\n",
    "    \n",
    "    # Initialize state estimates and covariance matrices (scalars)\n",
    "    state_estimates = {f'mote{i}': 20.0 for i in range(1, num_nodes + 1)}  # Initial state estimates\n",
    "    P = {f'mote{i}': 0.5 for i in range(1, num_nodes + 1)}  # Initial covariance matrices (scalars)\n",
    "    last_observations = {f'mote{i}': 20.0 for i in range(1, num_nodes + 1)}  # Initialize last observed values\n",
    "    last_update_times = {f'mote{i}': 0 for i in range(1, num_nodes + 1)}  # Time when the last measurement was received\n",
    "\n",
    "    cumulative_reward = 0  # Track total cumulative reward\n",
    "    cumulative_rewards = []  # Store cumulative average reward over time\n",
    "\n",
    "    # List to log the selected nodes and their covariance error\n",
    "    polled_nodes_info = []\n",
    "\n",
    "    for idx, row in pivot_df.iterrows():\n",
    "        current_time_step = idx\n",
    "\n",
    "        # **Step 1**: Select nodes with the highest covariance\n",
    "        covariance_traces = {mote: P[f'mote{mote}'] for mote in range(1, num_nodes + 1)}\n",
    "        sorted_motes_by_trace = sorted(covariance_traces, key=covariance_traces.get, reverse=True)\n",
    "        top_covariance_nodes = sorted_motes_by_trace[:M]\n",
    "\n",
    "        # **Step 2**: Further filter nodes with covariance > threshold\n",
    "        nodes_to_poll = []\n",
    "        for mote in top_covariance_nodes:\n",
    "            column_name = f'mote{mote}'\n",
    "            if column_name in row:\n",
    "                covariance = P[column_name]  # Current covariance value\n",
    "\n",
    "                # Include node if covariance > threshold\n",
    "                if covariance > covariance_threshold:\n",
    "                    nodes_to_poll.append(mote)\n",
    "\n",
    "                    # Log the polled node and its covariance\n",
    "                    polled_nodes_info.append({\n",
    "                        \"time_step\": current_time_step,\n",
    "                        \"node\": mote,\n",
    "                        \"covariance\": covariance\n",
    "                    })\n",
    "\n",
    "        total_reward_for_round = 0\n",
    "\n",
    "        # Process the selected nodes\n",
    "        for mote in nodes_to_poll:\n",
    "            column_name = f'mote{mote}'\n",
    "            if column_name in row:\n",
    "                measured_value = row[column_name]  # Current measurement\n",
    "                last_observed_value = last_observations[column_name]  # Last observed value\n",
    "\n",
    "                # Reward or penalty based on the difference between the last observation and the current measured value\n",
    "                if abs(last_observed_value - measured_value) > theta:\n",
    "                    total_reward_for_round += 1  # Reward\n",
    "                else:\n",
    "                    total_reward_for_round += penalty  # Penalty\n",
    "\n",
    "                # Prediction step\n",
    "                previous_state = state_estimates[column_name]\n",
    "                previous_P = P[column_name]\n",
    "                predicted_covariance = previous_P + Q  # Update covariance with process noise\n",
    "\n",
    "                # Kalman filter update step\n",
    "                K = predicted_covariance / (predicted_covariance + R)  # Kalman gain\n",
    "                updated_state = previous_state + K * (measured_value - previous_state)  # Update state estimate\n",
    "                updated_covariance = (1 - K) * predicted_covariance  # Update covariance\n",
    "\n",
    "                # Update state estimates, covariance, and last observed value\n",
    "                state_estimates[column_name] = updated_state\n",
    "                P[column_name] = updated_covariance\n",
    "                last_observations[column_name] = measured_value  # Update last observed value\n",
    "                last_update_times[column_name] = current_time_step  # Update last update time\n",
    "\n",
    "        # Update cumulative reward\n",
    "        cumulative_reward += total_reward_for_round\n",
    "        cumulative_rewards.append(cumulative_reward / (idx + 1))  # Compute cumulative average reward\n",
    "\n",
    "    # Convert polled nodes info to a DataFrame for analysis\n",
    "    polled_nodes_info_df = pd.DataFrame(polled_nodes_info)\n",
    "    polled_nodes_info_df.to_csv('polled_nodes_covariance_check.csv', index=False)  # Save to CSV\n",
    "\n",
    "    return cumulative_rewards, polled_nodes_info_df\n",
    "\n",
    "# Parameters\n",
    "M = 10  # Number of nodes to poll\n",
    "theta = 0.5  # Threshold for significant difference\n",
    "penalty = -0.5  # Penalty for insignificant difference\n",
    "Q = 0.001  # Process noise covariance (scalar)\n",
    "R = 0.1  # Measurement noise covariance (scalar)\n",
    "covariance_threshold = 0.5  # Threshold for covariance error\n",
    "\n",
    "# Run the simulation\n",
    "cumulative_rewards_kalman, polled_nodes_info = run_simulation_kalman_with_covariance_check(\n",
    "    pivot_df, num_nodes, M, theta, penalty, covariance_threshold\n",
    ")\n",
    "\n",
    "# View the polled nodes' info DataFrame\n",
    "print(polled_nodes_info)\n",
    "\n",
    "# Save cumulative rewards to CSV\n",
    "pd.DataFrame(cumulative_rewards_kalman, columns=[\"cumulative_reward\"]).to_csv('cumulative_rewards_kalman_covariance.csv', index=False)\n",
    "\n",
    "# Plot cumulative average reward over time\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(cumulative_rewards_kalman) + 1), cumulative_rewards_kalman, label='Kalman Filter with Covariance Check')\n",
    "plt.xlabel('Time Steps', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Cumulative Average Reward', fontsize=14, fontweight='bold')\n",
    "plt.title('Kalman Filter with Covariance Check - Cumulative Average Reward', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.xlim(0, len(cumulative_rewards_kalman))\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('kalman_covariance_check_reward.png')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def run_simulation_kalman_with_covariance_check(pivot_df, num_nodes, M, theta, penalty, covariance_threshold):\n",
    "    \"\"\"Runs the simulation using Kalman filter (1D) and computes cumulative average reward with covariance threshold check.\"\"\"\n",
    "    \n",
    "    # Initialize state estimates and covariance matrices (scalars)\n",
    "    state_estimates = {f'mote{i}': 20.0 for i in range(1, num_nodes + 1)}  # Initial state estimates\n",
    "    P = {f'mote{i}': 1 for i in range(1, num_nodes + 1)}  # Initial covariance matrices (scalars)\n",
    "    last_observations = {f'mote{i}': 0.0 for i in range(1, num_nodes + 1)}  # Initialize last observed values\n",
    "    last_update_times = {f'mote{i}': 0 for i in range(1, num_nodes + 1)}  # Time when the last measurement was received\n",
    "\n",
    "    cumulative_reward = 0  # Track total cumulative reward\n",
    "    cumulative_rewards = []  # Store cumulative average reward over time\n",
    "\n",
    "    # List to log the selected nodes and their covariance error\n",
    "    polled_nodes_info = []\n",
    "\n",
    "    for idx, row in pivot_df.iterrows():\n",
    "        current_time_step = idx\n",
    "\n",
    "        # **Step 1**: Select nodes with the highest covariance\n",
    "        covariance_traces = {mote: P[f'mote{mote}'] for mote in range(1, num_nodes + 1)}\n",
    "        sorted_motes_by_trace = sorted(covariance_traces, key=covariance_traces.get, reverse=True)\n",
    "        top_covariance_nodes = sorted_motes_by_trace[:M]\n",
    "\n",
    "        # **Step 2**: Further filter nodes with covariance > threshold\n",
    "        nodes_to_poll = []\n",
    "        for mote in top_covariance_nodes:\n",
    "            column_name = f'mote{mote}'\n",
    "            if column_name in row:\n",
    "                covariance = P[column_name]  # Current covariance value\n",
    "\n",
    "                # Include node if covariance > threshold\n",
    "                if covariance > covariance_threshold:\n",
    "                    nodes_to_poll.append(mote)\n",
    "\n",
    "                    # Log the polled node and its covariance\n",
    "                    polled_nodes_info.append({\n",
    "                        \"time_step\": current_time_step,\n",
    "                        \"node\": mote,\n",
    "                        \"covariance\": covariance\n",
    "                    })\n",
    "\n",
    "        total_reward_for_round = 0\n",
    "\n",
    "        # Process the selected nodes\n",
    "        for mote in nodes_to_poll:\n",
    "            column_name = f'mote{mote}'\n",
    "            if column_name in row:\n",
    "                measured_value = row[column_name]  # Current measurement\n",
    "                last_observed_value = last_observations[column_name]  # Last observed value\n",
    "\n",
    "                # Reward or penalty based on the difference between the last observation and the current measured value\n",
    "                if abs(last_observed_value - measured_value) > theta:\n",
    "                    total_reward_for_round += 1  # Reward\n",
    "                else:\n",
    "                    total_reward_for_round += penalty  # Penalty\n",
    "\n",
    "                # Prediction step\n",
    "                previous_state = state_estimates[column_name]\n",
    "                previous_P = P[column_name]\n",
    "                predicted_covariance = previous_P + Q  # Update covariance with process noise\n",
    "\n",
    "                # Kalman filter update step\n",
    "                K = predicted_covariance / (predicted_covariance + R)  # Kalman gain\n",
    "                updated_state = previous_state + K * (measured_value - previous_state)  # Update state estimate\n",
    "                updated_covariance = (1 - K) * predicted_covariance  # Update covariance\n",
    "\n",
    "                # Update state estimates, covariance, and last observed value\n",
    "                state_estimates[column_name] = updated_state\n",
    "                P[column_name] = updated_covariance\n",
    "                last_observations[column_name] = measured_value  # Update last observed value\n",
    "                last_update_times[column_name] = current_time_step  # Update last update time\n",
    "\n",
    "        # Update cumulative reward\n",
    "        cumulative_reward += total_reward_for_round\n",
    "        cumulative_rewards.append(cumulative_reward / (idx + 1))  # Compute cumulative average reward\n",
    "\n",
    "    # Convert polled nodes info to a DataFrame for analysis\n",
    "    polled_nodes_info_df = pd.DataFrame(polled_nodes_info)\n",
    "    polled_nodes_info_df.to_csv('polled_nodes_covariance_check.csv', index=False)  # Save to CSV\n",
    "\n",
    "    return cumulative_rewards, polled_nodes_info_df\n",
    "\n",
    "# Parameters\n",
    "M = 10  # Number of nodes to poll\n",
    "theta = 0.5  # Threshold for significant difference\n",
    "penalty = -0.5  # Penalty for insignificant difference\n",
    "Q = 0.001  # Process noise covariance (scalar)\n",
    "R = 0.1  # Measurement noise covariance (scalar)\n",
    "covariance_threshold = 0.5  # Threshold for covariance error\n",
    "\n",
    "# Run the simulation\n",
    "cumulative_rewards_kalman, polled_nodes_info = run_simulation_kalman_with_covariance_check(\n",
    "    pivot_df, num_nodes, M, theta, penalty, covariance_threshold\n",
    ")\n",
    "\n",
    "# View the polled nodes' info DataFrame\n",
    "print(polled_nodes_info)\n",
    "\n",
    "# Save cumulative rewards to CSV\n",
    "pd.DataFrame(cumulative_rewards_kalman, columns=[\"cumulative_reward\"]).to_csv('cumulative_rewards_kalman_covariance.csv', index=False)\n",
    "\n",
    "# Plot cumulative average reward over time\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(cumulative_rewards_kalman) + 1), cumulative_rewards_kalman, label='Kalman Filter with Covariance Check')\n",
    "plt.xlabel('Time Steps', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Cumulative Average Reward', fontsize=14, fontweight='bold')\n",
    "plt.title('Kalman Filter with Covariance Check - Cumulative Average Reward', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.xlim(0, len(cumulative_rewards_kalman))\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('kalman_covariance_check_reward.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation_kalman_with_two_step_pooling(pivot_df, num_nodes, M, theta, penalty):\n",
    "    \"\"\"Runs the simulation using Kalman filter (1D) and computes cumulative average reward with two-step pooling.\"\"\"\n",
    "    \n",
    "    # Initialize state estimates and covariance matrices (scalars)\n",
    "    state_estimates = {f'mote{i}': 20.0 for i in range(1, num_nodes + 1)}  # Initial state estimates\n",
    "    P = {f'mote{i}': 0.5 for i in range(1, num_nodes + 1)}  # Initial covariance matrices (scalars)\n",
    "    last_observations = {f'mote{i}': 20.0 for i in range(1, num_nodes + 1)}  # Initialize last observed values\n",
    "    last_update_times = {f'mote{i}': 0 for i in range(1, num_nodes + 1)}  # Time when the last measurement was received\n",
    "\n",
    "    cumulative_reward = 0  # Track total cumulative reward\n",
    "    cumulative_rewards = []  # Store cumulative average reward over time\n",
    "\n",
    "    # List to log the selected nodes and their MSE\n",
    "    polled_nodes_info = []\n",
    "\n",
    "    for idx, row in pivot_df.iterrows():\n",
    "        current_time_step = idx\n",
    "\n",
    "        # **Step 1**: Select nodes with the highest covariance\n",
    "        covariance_traces = {mote: P[f'mote{mote}'] for mote in range(1, num_nodes + 1)}\n",
    "        sorted_motes_by_trace = sorted(covariance_traces, key=covariance_traces.get, reverse=True)\n",
    "        top_covariance_nodes = sorted_motes_by_trace[:M]\n",
    "\n",
    "        # **Step 2**: Further filter nodes by MSE threshold\n",
    "        nodes_to_poll = []\n",
    "        for mote in top_covariance_nodes:\n",
    "            column_name = f'mote{mote}'\n",
    "            if column_name in row:\n",
    "                last_observed_value = last_observations[column_name]\n",
    "                previous_state = state_estimates[column_name]\n",
    "\n",
    "                # Predict next state\n",
    "                predicted_state = previous_state  # No dynamics, so prediction is the previous state\n",
    "\n",
    "                # Calculate MSE (difference between last observation and predicted state)\n",
    "                mse = (last_observed_value - predicted_state) ** 2\n",
    "\n",
    "                # Include the node for polling only if MSE > 0\n",
    "                if mse > 0:\n",
    "                    nodes_to_poll.append(mote)\n",
    "\n",
    "                    # Log the polled node and its MSE\n",
    "                    polled_nodes_info.append({\n",
    "                        \"time_step\": current_time_step,\n",
    "                        \"node\": mote,\n",
    "                        \"mse\": mse,\n",
    "                        \"covariance\": covariance_traces[mote]\n",
    "                    })\n",
    "\n",
    "        total_reward_for_round = 0\n",
    "\n",
    "        # Process the selected nodes\n",
    "        for mote in nodes_to_poll:\n",
    "            column_name = f'mote{mote}'\n",
    "            if column_name in row:\n",
    "                measured_value = row[column_name]  # Current measurement\n",
    "                last_observed_value = last_observations[column_name]  # Last observed value\n",
    "\n",
    "                # Reward or penalty based on the difference between the last observation and the current measured value\n",
    "                if abs(last_observed_value - measured_value) > theta:\n",
    "                    total_reward_for_round += 1  # Reward\n",
    "                else:\n",
    "                    total_reward_for_round += penalty  # Penalty\n",
    "\n",
    "                # Prediction step\n",
    "                previous_state = state_estimates[column_name]\n",
    "                previous_P = P[column_name]\n",
    "                predicted_covariance = previous_P + Q  # Update covariance with process noise\n",
    "\n",
    "                # Kalman filter update step\n",
    "                K = predicted_covariance / (predicted_covariance + R)  # Kalman gain\n",
    "                updated_state = previous_state + K * (measured_value - previous_state)  # Update state estimate\n",
    "                updated_covariance = (1 - K) * predicted_covariance  # Update covariance\n",
    "\n",
    "                # Update state estimates, covariance, and last observed value\n",
    "                state_estimates[column_name] = updated_state\n",
    "                P[column_name] = updated_covariance\n",
    "                last_observations[column_name] = measured_value  # Update last observed value\n",
    "                last_update_times[column_name] = current_time_step  # Update last update time\n",
    "\n",
    "        # Update cumulative reward\n",
    "        cumulative_reward += total_reward_for_round\n",
    "        cumulative_rewards.append(cumulative_reward / (idx + 1))  # Compute cumulative average reward\n",
    "\n",
    "    # Convert polled nodes info to a DataFrame for analysis\n",
    "    polled_nodes_info_df = pd.DataFrame(polled_nodes_info)\n",
    "    polled_nodes_info_df.to_csv('polled_nodes_two_step_pooling.csv', index=False)  # Save to CSV\n",
    "\n",
    "    return cumulative_rewards, polled_nodes_info_df\n",
    "\n",
    "# Parameters\n",
    "M = 10  # Number of nodes to poll\n",
    "theta = 0.5  # Threshold for significant difference\n",
    "penalty = -0.5  # Penalty for insignificant difference\n",
    "Q = 0.001  # Process noise covariance (scalar)\n",
    "R = 0.1  # Measurement noise covariance (scalar)\n",
    "\n",
    "# Run the simulation\n",
    "cumulative_rewards_kalman, polled_nodes_info = run_simulation_kalman_with_two_step_pooling(\n",
    "    pivot_df, num_nodes, M, theta, penalty\n",
    ")\n",
    "\n",
    "# View the polled nodes' info DataFrame\n",
    "print(polled_nodes_info)\n",
    "\n",
    "# Save cumulative rewards to CSV\n",
    "pd.DataFrame(cumulative_rewards_kalman, columns=[\"cumulative_reward\"]).to_csv('cumulative_rewards_kalman_two_step.csv', index=False)\n",
    "\n",
    "# Plot cumulative average reward over time\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(cumulative_rewards_kalman) + 1), cumulative_rewards_kalman, label='Kalman Filter with Two-Step Pooling')\n",
    "plt.xlabel('Time Steps', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Cumulative Average Reward', fontsize=14, fontweight='bold')\n",
    "plt.title('Kalman Filter with Two-Step Pooling - Cumulative Average Reward', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.xlim(0, len(cumulative_rewards_kalman))\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('kalman_two_step_pooling_reward.png')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'mote51'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'mote51'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cumulative_rewards\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Run the simulation\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m cumulative_rewards_whittle \u001b[38;5;241m=\u001b[39m \u001b[43mrun_simulation_whittle_aoi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpivot_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Save cumulative rewards to CSV\u001b[39;00m\n\u001b[0;32m     60\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(cumulative_rewards_whittle, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcumulative_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcumulative_rewards_whittle.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[4], line 37\u001b[0m, in \u001b[0;36mrun_simulation_whittle_aoi\u001b[1;34m(pivot_df, num_nodes, M, theta, penalty)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes_to_poll:\n\u001b[0;32m     36\u001b[0m     column_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmote\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 37\u001b[0m     current_value \u001b[38;5;241m=\u001b[39m \u001b[43mpivot_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     38\u001b[0m     last_value \u001b[38;5;241m=\u001b[39m last_polled_values[column_name]\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# Calculate reward or penalty\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\indexing.py:1183\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1181\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(com\u001b[38;5;241m.\u001b[39mapply_if_callable(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m key)\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m-> 1183\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_takeable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_tuple(key)\n\u001b[0;32m   1185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1186\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:4214\u001b[0m, in \u001b[0;36mDataFrame._get_value\u001b[1;34m(self, index, col, takeable)\u001b[0m\n\u001b[0;32m   4211\u001b[0m     series \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ixs(col, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   4212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m series\u001b[38;5;241m.\u001b[39m_values[index]\n\u001b[1;32m-> 4214\u001b[0m series \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_item_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4215\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_engine\n\u001b[0;32m   4217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, MultiIndex):\n\u001b[0;32m   4218\u001b[0m     \u001b[38;5;66;03m# CategoricalIndex: Trying to use the engine fastpath may give incorrect\u001b[39;00m\n\u001b[0;32m   4219\u001b[0m     \u001b[38;5;66;03m#  results if our categories are integers that dont match our codes\u001b[39;00m\n\u001b[0;32m   4220\u001b[0m     \u001b[38;5;66;03m# IntervalIndex: IntervalTree has no get_loc\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:4638\u001b[0m, in \u001b[0;36mDataFrame._get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   4633\u001b[0m res \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(item)\n\u001b[0;32m   4634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4635\u001b[0m     \u001b[38;5;66;03m# All places that call _get_item_cache have unique columns,\u001b[39;00m\n\u001b[0;32m   4636\u001b[0m     \u001b[38;5;66;03m#  pending resolution of GH#33047\u001b[39;00m\n\u001b[1;32m-> 4638\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4639\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ixs(loc, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   4641\u001b[0m     cache[item] \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'mote51'"
     ]
    }
   ],
   "source": [
    "# Function to simulate Whittle Index AoI with cumulative reward tracking\n",
    "M=5\n",
    "def run_simulation_whittle_aoi(pivot_df, num_nodes, M, theta, penalty):\n",
    "    cumulative_reward = 0  # Track total cumulative reward\n",
    "    cumulative_rewards = []  # Store cumulative average reward over time\n",
    "\n",
    "    last_update_whittle = np.zeros(num_nodes)  # Last update times for each node\n",
    "    last_polled_values = {f'mote{i}': 20.0 for i in range(1, num_nodes + 1)}  # Initial values for each node\n",
    "\n",
    "    def transmission_success(prob):\n",
    "        return np.random.rand() < prob\n",
    "\n",
    "    for t in range(len(pivot_df)):  # Loop over all time steps\n",
    "        whittle_indices = {}\n",
    "\n",
    "        # Compute Whittle indices for each node\n",
    "        for node in range(1, num_nodes + 1):\n",
    "            column_name = f'mote{node}'\n",
    "            delta_t_dynamic = t - last_update_whittle[node - 1]  # Time since last update (AoI)\n",
    "            current_aoi = delta_t_dynamic\n",
    "            future_aoi_passive = delta_t_dynamic + 1\n",
    "            future_aoi_active = 0  # Assume perfect transmission for now\n",
    "\n",
    "            q_passive = current_aoi + future_aoi_passive\n",
    "            q_active = current_aoi + future_aoi_active + 0  # No penalty for active action\n",
    "            whittle_indices[node] = q_passive - q_active\n",
    "\n",
    "        # Poll top M nodes with the highest Whittle index\n",
    "        nodes_to_poll = [node for node, index in whittle_indices.items() if index >= 0]\n",
    "        if len(nodes_to_poll) > M:\n",
    "            nodes_to_poll = sorted(nodes_to_poll, key=lambda node: whittle_indices[node], reverse=True)[:M]\n",
    "\n",
    "        round_reward = 0  # Reward for the current time step\n",
    "\n",
    "        for node in nodes_to_poll:\n",
    "            column_name = f'mote{node}'\n",
    "            current_value = pivot_df.loc[t, column_name]\n",
    "            last_value = last_polled_values[column_name]\n",
    "\n",
    "            # Calculate reward or penalty\n",
    "            if abs(current_value - last_value) > theta:\n",
    "                round_reward += 1  # Reward\n",
    "            else:\n",
    "                round_reward += penalty  # Penalty\n",
    "\n",
    "            # Update the last polled value\n",
    "            last_polled_values[column_name] = current_value\n",
    "            last_update_whittle[node - 1] = t  # Update last update time\n",
    "\n",
    "        # Update cumulative reward\n",
    "        cumulative_reward += round_reward\n",
    "        cumulative_rewards.append(cumulative_reward / (t + 1))  # Calculate cumulative average reward\n",
    "\n",
    "    return cumulative_rewards\n",
    "\n",
    "# Run the simulation\n",
    "cumulative_rewards_whittle = run_simulation_whittle_aoi(pivot_df, num_nodes, M, theta, penalty)\n",
    "\n",
    "# Save cumulative rewards to CSV\n",
    "pd.DataFrame(cumulative_rewards_whittle, columns=[\"cumulative_reward\"]).to_csv('cumulative_rewards_whittle.csv', index=False)\n",
    "\n",
    "# Plot cumulative average reward over time\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(cumulative_rewards_whittle) + 1), cumulative_rewards_whittle, label='Whittle AoII Cumulative Average Reward')\n",
    "plt.xlabel('Time Steps', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Cumulative Average Reward', fontsize=14, fontweight='bold')\n",
    "plt.title('Whittle AoII Cumulative Average Reward over Time', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.xlim(0, len(cumulative_rewards_whittle))\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('whittle_aoi_cumulative_average_reward.png')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Parameters\n",
    "columns = pivot_df.columns  # Get the actual column names from the dataset\n",
    "num_nodes = len(columns)  # Total number of nodes based on columns in the dataset\n",
    "#M = 10  # Maximum number of nodes to poll in each time step\n",
    "theta = 0.5  # Threshold for significant difference\n",
    "penalty = -0.5  # Penalty for insignificant difference\n",
    "initial_value = 20  # Initial value for last polled\n",
    "num_time_steps = len(pivot_df)  # Total time steps based on the dataset\n",
    "\n",
    "# Initialize last polled values\n",
    "last_polled_values_rr = {column: initial_value for column in columns}\n",
    "\n",
    "def run_simulation_round_robin(pivot_df, columns, M, theta, penalty):\n",
    "    last_polled_values = {column: initial_value for column in columns}  # Last polled values\n",
    "    cumulative_reward = 0  # Total cumulative reward\n",
    "    cumulative_rewards = []  # Store cumulative rewards over time\n",
    "\n",
    "    for t in range(len(pivot_df)):  # Loop over all time steps\n",
    "        # Poll M nodes in cyclic order\n",
    "        nodes_rr = [columns[(t + i) % num_nodes] for i in range(M)]  # Poll M nodes in cyclic order\n",
    "        round_reward = 0  # Reward for the current time step\n",
    "\n",
    "        for node_rr in nodes_rr:\n",
    "            current_value = pivot_df.loc[t, node_rr]  # Current value for the node\n",
    "            last_value = last_polled_values[node_rr]\n",
    "\n",
    "            # Calculate reward or penalty\n",
    "            if abs(current_value - last_value) > theta:\n",
    "                round_reward += 1  # Reward\n",
    "            else:\n",
    "                round_reward += penalty  # Penalty\n",
    "\n",
    "            # Update the last polled value\n",
    "            last_polled_values[node_rr] = current_value\n",
    "\n",
    "        # Update cumulative reward\n",
    "        cumulative_reward += round_reward\n",
    "        cumulative_rewards.append(cumulative_reward / (t + 1))  # Append cumulative average reward\n",
    "\n",
    "    return cumulative_rewards\n",
    "\n",
    "# Run Round Robin simulation with perfect transmission\n",
    "cumulative_rewards_rr = run_simulation_round_robin(pivot_df, columns, M, theta, penalty)\n",
    "\n",
    "# Save cumulative rewards to CSV\n",
    "pd.DataFrame(cumulative_rewards_rr, columns=[\"cumulative_reward\"]).to_csv('cumulative_rewards_round_robin.csv', index=False)\n",
    "\n",
    "# Plot cumulative average reward over time\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(cumulative_rewards_rr) + 1), cumulative_rewards_rr, label='Round Robin Cumulative Average Reward')\n",
    "plt.xlabel('Time Steps', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Cumulative Average Reward', fontsize=14, fontweight='bold')\n",
    "plt.title('Round Robin Cumulative Average Reward over Time', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.xlim(0, len(cumulative_rewards_rr))\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('round_robin_cumulative_average_reward.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cumulative_rewards_oracle.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load cumulative rewards from CSV files\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m cumulative_rewards_oracle \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcumulative_rewards_oracle.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcumulative_reward\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      7\u001b[0m cumulative_rewards_rr \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcumulative_rewards_round_robin.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcumulative_reward\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      8\u001b[0m cumulative_rewards_whittle_aoii \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcumulative_rewards_whittle_aoii.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcumulative_reward\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cumulative_rewards_oracle.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load cumulative rewards from CSV files\n",
    "cumulative_rewards_oracle = pd.read_csv('cumulative_rewards_oracle.csv')['cumulative_reward'].values\n",
    "cumulative_rewards_rr = pd.read_csv('cumulative_rewards_round_robin.csv')['cumulative_reward'].values\n",
    "cumulative_rewards_whittle_aoii = pd.read_csv('cumulative_rewards_whittle_aoii.csv')['cumulative_reward'].values\n",
    "cumulative_rewards_whittle_aoi = pd.read_csv('cumulative_rewards_whittle_aoi.csv')['cumulative_reward'].values\n",
    "cumulative_rewards_kalman = pd.read_csv('cumulative_rewards_kalman.csv')['cumulative_reward'].values\n",
    "\n",
    "# Calculate cumulative average rewards for each technique\n",
    "cumulative_average_reward_oracle = np.cumsum(cumulative_rewards_oracle) / (np.arange(len(cumulative_rewards_oracle)) + 1)\n",
    "cumulative_average_reward_rr = np.cumsum(cumulative_rewards_rr) / (np.arange(len(cumulative_rewards_rr)) + 1)\n",
    "cumulative_average_reward_waoii = np.cumsum(cumulative_rewards_whittle_aoii) / (np.arange(len(cumulative_rewards_whittle_aoii)) + 1)\n",
    "cumulative_average_reward_waoi = np.cumsum(cumulative_rewards_whittle_aoi) / (np.arange(len(cumulative_rewards_whittle_aoi)) + 1)\n",
    "cumulative_average_reward_kalman = np.cumsum(cumulative_rewards_kalman) / (np.arange(len(cumulative_rewards_kalman)) + 1)\n",
    "\n",
    "# Calculate regret for each technique by subtracting from the oracle's cumulative average reward\n",
    "regret_rr = cumulative_average_reward_oracle - cumulative_average_reward_rr\n",
    "regret_waoii = cumulative_average_reward_oracle - cumulative_average_reward_waoii\n",
    "regret_waoi = cumulative_average_reward_oracle - cumulative_average_reward_waoi\n",
    "regret_kalman = cumulative_average_reward_oracle - cumulative_average_reward_kalman\n",
    "\n",
    "# Plot the regret over time\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(len(regret_rr)), regret_rr, label='RR Regret', linewidth=2)\n",
    "plt.plot(range(len(regret_waoii)), regret_waoii, label='W-AoII Regret', linewidth=2)\n",
    "plt.plot(range(len(regret_waoi)), regret_waoi, label='W-AoI Regret', linewidth=2)\n",
    "plt.plot(range(len(regret_kalman)), regret_kalman, label='KF Regret', linewidth=2)\n",
    "\n",
    "# Label the plot\n",
    "plt.xlabel('Time Step', fontsize=16)\n",
    "plt.ylabel('Regret', fontsize=16)\n",
    "plt.title('Regret Comparison Across Techniques', fontsize=16, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.7)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.xlim(0, 5000)  # Adjust this limit as needed based on your data\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('comparison_regret_over_time.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
