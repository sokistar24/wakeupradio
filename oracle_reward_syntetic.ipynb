{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#pivot_df = pd.read_csv('synthetic_scenario_one.csv')\n",
    "pivot_df = pd.read_csv('synthetic_scenario_three.csv')\n",
    "\n",
    "# Fill missing values with the column mean\n",
    "pivot_df = pivot_df.apply(lambda x: x.fillna(x.mean()), axis=0)\n",
    "\n",
    "# Parameters\n",
    "M = 2  # Maximum number of nodes that can be polled\n",
    "theta = 0.5  # Threshold for reward condition\n",
    "penalty = -1.0  # Penalty for polling when difference is <= theta\n",
    "initial_value = 20  # Initial estimate for last polled values\n",
    "aoii_penalty = 1.0\n",
    "reward = 1.0\n",
    "\n",
    "# Initialize last polled values\n",
    "last_polled_values = {f'mote{i}': initial_value for i in range(1, 51)}\n",
    "\n",
    "def run_oracle_simulation(M):\n",
    "    last_update_times = {f'mote{i}': 0 for i in range(1, 51)}\n",
    "    cumulative_reward = 0  # Total cumulative reward\n",
    "    cumulative_rewards = []  # Store cumulative average reward over time\n",
    "    valuable_sensor_data = []\n",
    "\n",
    "    for t in range(len(pivot_df)):  # Loop over each time step\n",
    "        current_time_step = t\n",
    "        rewards = {}\n",
    "\n",
    "        # Calculate rewards for each mote\n",
    "        for mote in last_update_times:\n",
    "            current_value = pivot_df.loc[current_time_step, mote]\n",
    "            last_value = last_polled_values[mote]\n",
    "\n",
    "            # Reward is the difference between the current and last value\n",
    "            rewards[mote] = abs(current_value - last_value)\n",
    "\n",
    "        # Filter nodes with rewards greater than the threshold\n",
    "        eligible_nodes = {mote: reward for mote, reward in rewards.items() if reward > theta}\n",
    "\n",
    "        # Select the top M nodes among eligible nodes\n",
    "        top_m_nodes = sorted(eligible_nodes, key=eligible_nodes.get, reverse=True)[:M]\n",
    "\n",
    "        total_reward_for_round = 0\n",
    "\n",
    "        # Process the selected nodes\n",
    "        for mote in top_m_nodes:\n",
    "            measured_value = pivot_df.loc[current_time_step, mote]\n",
    "            last_value = last_polled_values[mote]\n",
    "\n",
    "            # Calculate reward or penalty\n",
    "            if abs(measured_value - last_value) > theta:\n",
    "                total_reward_for_round += reward  # Reward\n",
    "            else:\n",
    "                total_reward_for_round += penalty  # Penalty\n",
    "\n",
    "        # Divide total reward for the round by M to standardize\n",
    "        if M > 0:  # Avoid division by zero\n",
    "            total_reward_for_round /= M\n",
    "\n",
    "        # Update cumulative reward and store it\n",
    "        cumulative_reward += total_reward_for_round\n",
    "        cumulative_rewards.append(cumulative_reward / (t + 1))  # Average cumulative reward over time\n",
    "\n",
    "    return cumulative_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to simulate Whittle AoII with rewards\n",
    "def run_simulation_whittle_aoii(pivot_df, columns, M, theta, penalty):\n",
    "    cumulative_reward = 0  # Track total cumulative reward\n",
    "    cumulative_rewards = []  # Store cumulative average reward over time\n",
    "    last_update_times = {col: 0 for col in columns}  # Last update time for each node\n",
    "    state_node = {col: np.array([20.0, 0.1]) for col in columns}  # Node states\n",
    "\n",
    "    for t in range(len(pivot_df)):\n",
    "        # Step 1: Compute Whittle indices for each node based on AoII\n",
    "        whittle_indices = {}\n",
    "        for col in columns:\n",
    "            delta_t_dynamic = t - last_update_times[col]  # Time since last update\n",
    "            last_state_value, last_rate_of_change = state_node[col]\n",
    "            measured_value = pivot_df.loc[t, col]\n",
    "\n",
    "            # Predict AoII\n",
    "            current_aoii = abs(measured_value - last_state_value)  # Example AoII calculation\n",
    "            future_aoii_passive = current_aoii + delta_t_dynamic  # Assume AoII increases passively\n",
    "            future_aoii_active = 0  # AoII resets to 0 if polled\n",
    "            q_passive = current_aoii + future_aoii_passive\n",
    "            q_active = current_aoii + future_aoii_active + penalty\n",
    "\n",
    "            # Calculate Whittle index\n",
    "            whittle_indices[col] = q_passive - q_active\n",
    "\n",
    "        # Step 2: Select top M nodes to poll based on Whittle indices\n",
    "        nodes_to_poll = sorted(whittle_indices, key=whittle_indices.get, reverse=True)[:M]\n",
    "\n",
    "        # Step 3: Poll selected nodes and calculate rewards\n",
    "        for col in nodes_to_poll:\n",
    "            measured_value = pivot_df.loc[t, col]\n",
    "            last_state_value, last_rate_of_change = state_node[col]\n",
    "\n",
    "            # Calculate reward after polling\n",
    "            reward = calculate_reward(measured_value, last_state_value, theta, penalty)\n",
    "            cumulative_reward += reward  # Update cumulative reward\n",
    "\n",
    "            # Update node state and last update time\n",
    "            state_node[col] = update_node_state_dewma(\n",
    "                measured_value, last_state_value, last_rate_of_change, delta_t=1, beta_1=beta_1, beta_2=beta_2\n",
    "            )\n",
    "            last_update_times[col] = t\n",
    "\n",
    "        # Step 4: Calculate cumulative average reward\n",
    "        cumulative_rewards.append(cumulative_reward / (t + 1))\n",
    "\n",
    "    return cumulative_rewards\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
